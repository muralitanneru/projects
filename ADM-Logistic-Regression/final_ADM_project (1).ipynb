{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjrq4B9b_IIi"
      },
      "source": [
        "# DataSet load and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnTDEpzEJG4R",
        "outputId": "6c0b43f2-73c0-44a9-a70c-a846fefa9680"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "# Clean text data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bq4zuUiJVtX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFbB4NfkJXdW",
        "outputId": "314cd044-616a-4714-f90a-8e6f2ebcd824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyI853wcf7ZG"
      },
      "source": [
        "Load the Train, Test and Validation Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBREnZ3pJIHQ"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"/content/drive/MyDrive/DataSet/data_processed_train.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/DataSet/data_processed_test.csv\")\n",
        "val_data = pd.read_csv(\"/content/drive/MyDrive/DataSet/data_processed_validate.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vczOia83YeyF"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # # Remove urls\n",
        "    # text = re.sub(r'http\\S+', '', text)\n",
        "    # # Remove mentions\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "    # # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # # Remove stopwords and lemmatize\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "train_data['text'] = train_data['text'].apply(clean_text)\n",
        "test_data['text'] = test_data['text'].apply(clean_text)\n",
        "val_data['text'] = val_data['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n2VV8joJLMe",
        "outputId": "ee00b4a4-4b9b-401e-95e4-369bd23ba2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 2)\n",
            "(5000, 2)\n",
            "(2000, 2)\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(val_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVvbq0FQgJiz"
      },
      "source": [
        "Check the data set is balanced or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4gKZCVUgjYV",
        "outputId": "18ae0895-955f-4f9c-be90-9a6e71609843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1    5014\n",
            " 1    4986\n",
            "Name: target, dtype: int64\n",
            "-1    2521\n",
            " 1    2479\n",
            "Name: target, dtype: int64\n",
            "-1    1012\n",
            " 1     988\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_data['target'].value_counts())\n",
        "print(test_data['target'].value_counts())\n",
        "print(val_data['target'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARM_4MEBgTjI"
      },
      "source": [
        "Check Null Values present or not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIZPicazJ33g",
        "outputId": "c9abbe89-6872-4017-a044-1cd49a93145e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target    0\n",
            "text      0\n",
            "dtype: int64\n",
            "target    0\n",
            "text      0\n",
            "dtype: int64\n",
            "target    0\n",
            "text      0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_data.isnull().sum())\n",
        "print(test_data.isnull().sum())\n",
        "print(val_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d7N0sTsK31i"
      },
      "outputs": [],
      "source": [
        "# train_data = train_data.dropna()\n",
        "# test_data = test_data.dropna()\n",
        "# val_data = val_data.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVNu0C5TgbCw"
      },
      "source": [
        "# Balancing the DataSets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcYrTcGStISi"
      },
      "outputs": [],
      "source": [
        "# Balancing train data\n",
        "counts = train_data['target'].value_counts()\n",
        "min_count = counts.min()\n",
        "\n",
        "train_data = pd.concat([train_data[train_data['target'] == t].sample(min_count) for t in counts.index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ_SJdETuBUc"
      },
      "outputs": [],
      "source": [
        "# balancing validation dataset\n",
        "counts = val_data['target'].value_counts()\n",
        "min_count = counts.min()\n",
        "\n",
        "val_data = pd.concat([val_data[val_data['target'] == t].sample(min_count) for t in counts.index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJpFuNYk1hX2"
      },
      "outputs": [],
      "source": [
        "# balancing Test dataset\n",
        "counts = test_data['target'].value_counts()\n",
        "min_count = counts.min()\n",
        "\n",
        "test_data = pd.concat([test_data[test_data['target'] == t].sample(min_count) for t in counts.index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkKlZCRHK8s0",
        "outputId": "59135074-17e5-4c2d-c0dd-15c0fbde2143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9972, 2)\n",
            "(4958, 2)\n",
            "(1976, 2)\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(val_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feZstQGKgj1h"
      },
      "source": [
        "To check datasets are balanced or not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGvTvz8KKhM5",
        "outputId": "6a7fc28f-bc86-4c74-a335-91e60d3bffd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1    4986\n",
            " 1    4986\n",
            "Name: target, dtype: int64\n",
            "-1    2479\n",
            " 1    2479\n",
            "Name: target, dtype: int64\n",
            "-1    988\n",
            " 1    988\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_data['target'].value_counts())\n",
        "print(test_data['target'].value_counts())\n",
        "print(val_data['target'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMnP3PSgCMvI"
      },
      "source": [
        "# **Predictions Using CountVectorizer** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzNJP0z-M0Qr"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "# Create document term matrix using CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_train_count = count_vectorizer.fit_transform(train_data['text'])\n",
        "\n",
        "X_test_count = count_vectorizer.transform(test_data['text'])\n",
        "\n",
        "# Define the pipeline for CountVectorizer\n",
        "pipeline_count = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the model using CountVectorizer\n",
        "pipeline_count.fit(X_train_count, train_data['target'])\n",
        "end_time = time.time()\n",
        "print(\"Time taken to train the model: \", end_time - start_time, \" seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred_count = pipeline_count.predict(X_test_count)\n",
        "end_time = time.time()\n",
        "print(\"Time taken to test the model: \", end_time - start_time, \" seconds\")\n",
        "f1_score_count = f1_score(test_data['target'], y_pred_count)\n",
        "\n",
        "# Print the confusion matrix with labels\n",
        "\n",
        "cm = confusion_matrix(test_data['target'], y_pred_count)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Print the f1 score for CountVectorizer and TfidfVectorizer\n",
        "print(\"f1 score using CountVectorizer: \", f1_score_count)\n",
        "\n",
        "\n",
        "# Time taken to train the model:  1.951298475265503  seconds\n",
        "# Time taken to test the model:  0.0012433528900146484  seconds\n",
        "# Confusion Matrix:\n",
        "# [[1782  697]\n",
        "#  [ 668 1811]]\n",
        "# f1 score using CountVectorizer:  0.726288349709244"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUrSBkNGCaQQ"
      },
      "source": [
        "# Predictions Using TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWOI23UXM0eR"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "# Create document term matrix using TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['text'])\n",
        "\n",
        "\n",
        "# Define the pipeline for TfidfVectorizer\n",
        "pipeline_tfidf = Pipeline([\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the model using TfidfVectorizer\n",
        "pipeline_tfidf.fit(X_train_tfidf, train_data['target'])\n",
        "end_time = time.time()\n",
        "print(\"Time taken to train the model: \", end_time - start_time, \" seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['text'])\n",
        "y_pred_tfidf = pipeline_tfidf.predict(X_test_tfidf)\n",
        "end_time = time.time()\n",
        "print(\"Time taken to test the model: \", end_time - start_time, \" seconds\")\n",
        "\n",
        "f1_score_tfidf = f1_score(test_data['target'], y_pred_tfidf)\n",
        "\n",
        "# precision, recall, f1_score, _ = precision_recall_fscore_support(test_data['target'], y_pred_tfidf)\n",
        "\n",
        "# print(\"Precision:\", precision)\n",
        "# print(\"Recall:\", recall)\n",
        "# print(\"F1 score:\", f1_score)\n",
        "\n",
        "cm = confusion_matrix(test_data['target'], y_pred_tfidf)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "\n",
        "print(\"f1 score using TfidfVectorizer: \", f1_score_tfidf)\n",
        "\n",
        "\n",
        "# Time taken to train the model:  0.7965173721313477  seconds\n",
        "# Time taken to test the model:  0.2213137149810791  seconds\n",
        "# Confusion Matrix:\n",
        "# [[1798  681]\n",
        "#  [ 664 1815]]\n",
        "# f1 score using TfidfVectorizer:  0.7296482412060302"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TjVRsENCfux"
      },
      "source": [
        "# Hyper parameter tunning for Countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fbRwYzuM0pk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create document term matrix using CountVectorizer\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "\n",
        "# Define the pipeline for CountVectorizer\n",
        "# Define the pipeline for CountVectorizer with hyperparameters to tune\n",
        "pipeline_count = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Define hyperparameters to tune for CountVectorizer\n",
        "param_grid_count = {    \n",
        "    'clf__C': np.logspace(-1, 1, 20),\n",
        "    'clf__penalty': ['l1','l2'],\n",
        "    'clf__max_iter': [10,100],\n",
        "    'clf__solver': ['liblinear']\n",
        "}\n",
        "# param_grid_count = {\n",
        "#     'clf__C': np.logspace(-3, 3, 7),\n",
        "#     'clf__penalty': ['l1', 'l2'],\n",
        "#     'clf__dual': [False, True],\n",
        "#     'clf__fit_intercept': [True, False],\n",
        "#     'clf__intercept_scaling': [1, 2, 3],\n",
        "#     'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "#     'clf__max_iter': [100],\n",
        "#     'clf__class_weight': [None, 'balanced']\n",
        "# }\n",
        "\n",
        "# Perform hyperparameter tuning using GridSearchCV with the validation set\n",
        "grid_search_count = GridSearchCV(pipeline_count, param_grid=param_grid_count, cv=5)\n",
        "grid_search_count.fit(val_data['text'], val_data['target'])\n",
        "\n",
        "for i in range(len(grid_search_count.cv_results_['params'])):\n",
        "    print(grid_search_count.cv_results_['params'][i], \" -> \", grid_search_count.cv_results_['mean_test_score'][i])\n",
        "\n",
        "# Print the best hyperparameters for CountVectorizer\n",
        "print(\"Best hyperparameters for CountVectorizer: \", grid_search_count.best_params_)\n",
        "\n",
        "# Make predictions using the best CountVectorizer model on the validation set\n",
        "best_count_model = grid_search_count.best_estimator_\n",
        "print(best_count_model)\n",
        "\n",
        "y_pred = best_count_model.predict(val_data['text'])\n",
        "\n",
        "accuracy = accuracy_score(val_data['target'], y_pred)\n",
        "\n",
        "# Print the accuracy of the best CountVectorizer model on the validation set\n",
        "print(\"Accuracy of the best CountVectorizer model on the validation set: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHpE2QFxY2Wd"
      },
      "outputs": [],
      "source": [
        "# Best hyperparameters for CountVectorizer:  {'clf__C': 0.5455594781168519, 'clf__max_iter': 10, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
        "\n",
        "# Accuracy of the best CountVectorizer model on the validation set:  0.9271255060728745"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUSbfhqPCnig"
      },
      "source": [
        "# **Hyper Parameter tunninng for TfidfVectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7mIGHOsN-6T"
      },
      "outputs": [],
      "source": [
        "# Create document term matrix using TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
        "\n",
        "# Define the pipeline for TfidfVectorizer with hyperparameters to tune\n",
        "pipeline_tfidf = Pipeline([\n",
        "    ('vect', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Define hyperparameters to tune for TfidfVectorizer\n",
        "param_grid_tfidf = {\n",
        "    \n",
        "     #'clf__C': np.logspace(-1, 1, 100),\n",
        "#     # 'tfidf__max_df': [0.5, 0.75, 1.0],\n",
        "    'clf__C': [0.001,1,10],\n",
        "    'clf__max_iter': [10,100,200],\n",
        "    'clf__penalty': ['l1', 'l2'],\n",
        "    'clf__solver': [ 'liblinear']\n",
        "}\n",
        "# param_grid_tfidf = {\n",
        "#     'clf__C': np.logspace(-3, 3, 7),\n",
        "#     'clf__penalty': ['l1', 'l2'],\n",
        "#     'clf__dual': [False, True],\n",
        "#     'clf__fit_intercept': [True, False],\n",
        "#     'clf__intercept_scaling': [1, 2, 3],\n",
        "#     'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "#     'clf__max_iter': [100],\n",
        "#     'clf__class_weight': [None, 'balanced']\n",
        "# }\n",
        "\n",
        "# Perform hyperparameter tuning using GridSearchCV with the validation set\n",
        "grid_search_tfidf = GridSearchCV(pipeline_tfidf, param_grid=param_grid_tfidf, cv=5)\n",
        "grid_search_tfidf.fit(val_data['text'], val_data['target'])\n",
        "\n",
        "# Print accuracy for each set of hyperparameters\n",
        "print(\"Accuracy for each set of hyperparameters: \")\n",
        "for i in range(len(grid_search_tfidf.cv_results_['params'])):\n",
        "    print(grid_search_tfidf.cv_results_['params'][i], \" -> \", grid_search_tfidf.cv_results_['mean_test_score'][i])\n",
        "\n",
        "# Print the best hyperparameters for TfidfVectorizer\n",
        "print(\"Best hyperparameters for TfidfVectorizer: \", grid_search_tfidf.best_params_)\n",
        "\n",
        "# Make predictions using the best TfidfVectorizer model on the validation set\n",
        "best_tfidf_model = grid_search_tfidf.best_estimator_\n",
        "print(best_tfidf_model)\n",
        "\n",
        "\n",
        "# Print the best hyperparameters for TfidfVectorizer\n",
        "print(\"Best hyperparameters for TfidfVectorizer: \", grid_search_tfidf.best_params_)\n",
        "\n",
        "\n",
        "\n",
        "t_pred = best_tfidf_model.predict(val_data['text'])\n",
        "\n",
        "accuracy = accuracy_score(val_data['target'], t_pred)\n",
        "\n",
        "# Print the accuracy of the best CountVectorizer model on the validation set\n",
        "print(\"Accuracy of the best CountVectorizer model on the validation set: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9EdLGzKZCTq"
      },
      "outputs": [],
      "source": [
        "# Best hyperparameters for TfidfVectorizer:  {'clf__C': 1, 'clf__max_iter': 10, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
        "# Accuracy of the best CountVectorizer model on the validation set:  0.9129554655870445"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc4yCaXOf7En"
      },
      "source": [
        "# **CountVectorizer with best hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ5vFisKQk46"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "# Create document term matrix using CountVectorizer\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "X_train_count = count_vectorizer.fit_transform(train_data['text'])\n",
        "\n",
        "\n",
        "C = 0.42813323987193935\n",
        "max_iter = 10\n",
        "penalty = 'l2'\n",
        "solver = 'liblinear'\n",
        "\n",
        "pipeline_count = Pipeline([\n",
        "    ('clf', LogisticRegression(solver=solver, max_iter=max_iter, C=C, penalty=penalty))\n",
        "])\n",
        "\n",
        "\n",
        "# Define the pipeline for CountVectorizer\n",
        "# pipeline_count = Pipeline([\n",
        "#     ('clf', LogisticRegression(solver=grid_search_count.best_params_['clf__solver'],max_iter=grid_search_count.best_params_['clf__max_iter'], C=grid_search_count.best_params_['clf__C'], penalty=grid_search_count.best_params_['clf__penalty']))\n",
        "    \n",
        "# ])\n",
        "\n",
        "\n",
        "# Fit the model using CountVectorizer\n",
        "pipeline_count.fit(X_train_count, train_data['target'])\n",
        "end_time = time.time()\n",
        "print(\"Time taken to train the model: \", end_time - start_time, \" seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "X_test_count = count_vectorizer.transform(test_data['text'])\n",
        "y_pred_count = pipeline_count.predict(X_test_count)\n",
        "end_time = time.time()\n",
        "print(\"Time taken to test the model: \", end_time - start_time, \" seconds\")\n",
        "# Generate classification report for CountVectorizer\n",
        "report_count = classification_report(test_data['target'], y_pred_count)\n",
        "print(\"Classification report using CountVectorizer:\\n\", report_count)\n",
        "\n",
        "\n",
        "f1_score_count = f1_score(test_data['target'], y_pred_count)\n",
        "\n",
        "cm = confusion_matrix(test_data['target'], y_pred_count)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "precision_count = precision_score(test_data['target'], y_pred_count)\n",
        "recall_count = recall_score(test_data['target'], y_pred_count)\n",
        "\n",
        "print(\"Precision using CountVectorizer: \", precision_count)\n",
        "print(\"Recall using CountVectorizer: \", recall_count)\n",
        "\n",
        "\n",
        "# Print the f1 score for CountVectorizer \n",
        "print(\"f1 score using CountVectorizer: \", f1_score_count)\n",
        "\n",
        "\n",
        "# Time taken to train the model:  0.1705327033996582  seconds\n",
        "# Time taken to test the model:  0.07443976402282715  seconds\n",
        "# Classification report using CountVectorizer:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#           -1       0.73      0.72      0.73      2479\n",
        "#            1       0.72      0.74      0.73      2479\n",
        "\n",
        "#     accuracy                           0.73      4958\n",
        "#    macro avg       0.73      0.73      0.73      4958\n",
        "# weighted avg       0.73      0.73      0.73      4958\n",
        "\n",
        "# Confusion Matrix:\n",
        "# [[1780  699]\n",
        "#  [ 647 1832]]\n",
        "# Precision using CountVectorizer:  0.723824575266693\n",
        "# Recall using CountVectorizer:  0.7390076643807987\n",
        "# f1 score using CountVectorizer:  0.7313373253493014\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFo3jW-FgEfu"
      },
      "source": [
        "# **TfiDf Vectorizer with best hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1voHH5gQlKN"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "# Create document term matrix using TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['text'])\n",
        "\n",
        "\n",
        "# Define the pipeline for TfidfVectorizer with best hyperparameters\n",
        "pipeline_tfidf_lr_best = Pipeline([\n",
        "    \n",
        "    \n",
        "('clf', LogisticRegression(solver=grid_search_tfidf.best_params_['clf__solver'], \n",
        "                            C=grid_search_tfidf.best_params_['clf__C'], \n",
        "                            penalty=grid_search_tfidf.best_params_['clf__penalty'],\n",
        "                            max_iter=grid_search_tfidf.best_params_['clf__max_iter']))\n",
        "])\n",
        "\n",
        "# Fit the model using TfidfVectorizer with best hyperparameters\n",
        "pipeline_tfidf_lr_best.fit(X_train_tfidf, train_data['target'])\n",
        "end_time = time.time()\n",
        "print(\"Time taken to train the model: \", end_time - start_time, \" seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['text'])\n",
        "# Predict using the test data\n",
        "y_pred_tfidf = pipeline_tfidf_lr_best.predict(X_test_tfidf)\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Time taken to test the model: \", end_time - start_time, \" seconds\")\n",
        "\n",
        "# Generate classification report for TfidfVectorizer\n",
        "report_count = classification_report(test_data['target'], y_pred_tfidf)\n",
        "print(\"Classification report using tfidfVectorizer:\\n\", report_count)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(test_data['target'], y_pred_tfidf)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "precision_count = precision_score(test_data['target'], y_pred_tfidf)\n",
        "recall_count = recall_score(test_data['target'], y_pred_tfidf)\n",
        "\n",
        "print(\"Precision using TfIDfVectorizer: \", precision_count)\n",
        "print(\"Recall using TfiDfVectorizer: \", recall_count)\n",
        "\n",
        "\n",
        "# Calculate f1 score\n",
        "f1_score_tfidf = f1_score(test_data['target'], y_pred_tfidf)\n",
        "\n",
        "# Print the f1 score\n",
        "print(\"f1 score using TfidfVectorizer with best hyperparameters: \", f1_score_tfidf)\n",
        "\n",
        "# Time taken to train the model:  0.22537684440612793  seconds\n",
        "# Time taken to test the model:  0.13890576362609863  seconds\n",
        "# Classification report using tfidfVectorizer:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#           -1       0.73      0.73      0.73      2479\n",
        "#            1       0.73      0.73      0.73      2479\n",
        "\n",
        "#     accuracy                           0.73      4958\n",
        "#    macro avg       0.73      0.73      0.73      4958\n",
        "# weighted avg       0.73      0.73      0.73      4958\n",
        "\n",
        "# Confusion Matrix:\n",
        "# [[1799  680]\n",
        "#  [ 664 1815]]\n",
        "# Precision using TfIDfVectorizer:  0.7274549098196392\n",
        "# Recall using TfiDfVectorizer:  0.7321500605082695\n",
        "# f1 score using TfidfVectorizer with best hyperparameters:  0.7297949336550059\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMyndZxOQe0u"
      },
      "source": [
        "# **Measuring** Bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l-djsO5vsSB"
      },
      "source": [
        "Loading EEC corpus data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpxtLslfQdLx"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/DataSet/Equity-Evaluation-Corpus.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18-A_l1ndCEG"
      },
      "outputs": [],
      "source": [
        "data['Sentence'].nunique() # Number of unique sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9sV4x0QxfM9"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_2HGLsGl3RE"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpHdQWoxl3nv"
      },
      "outputs": [],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXKuvCHb_OSI"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fPsqPkzCAiF"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "#     # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "#     # Remove urls\n",
        "#     text = re.sub(r'http\\S+', '', text)\n",
        "#     # Remove mentions\n",
        "#     text = re.sub(r'@\\S+', '', text)\n",
        "#     # Remove numbers\n",
        "#     text = re.sub(r'\\d+', '', text)\n",
        "#     # Remove punctuation\n",
        "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
        "#     # Remove stopwords and lemmatize\n",
        "#     # text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "data['Sentence'] = data['Sentence'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5n9Hek7_lnf"
      },
      "outputs": [],
      "source": [
        "# Group the data by gender\n",
        "gender_groups = data.groupby('Gender')\n",
        "\n",
        "# Create separate datasets for male and female\n",
        "male_data = gender_groups.get_group('male')\n",
        "female_data = gender_groups.get_group('female')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ_epjJBvcHp"
      },
      "source": [
        "Both male and female data sets are same at every index, just gender is different in the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBtewwwekMFi"
      },
      "outputs": [],
      "source": [
        "print(male_data['Sentence'].iloc[100:105])\n",
        "print(female_data['Sentence'].iloc[100:105])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX9EJgDyif8o"
      },
      "source": [
        "Collect data set for each emotion from the male dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIrLYXtZor0Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "male_emotion_groups = male_data.groupby('Emotion')\n",
        "\n",
        "# Create separate datasets for male and female\n",
        "anger_male_data = male_emotion_groups.get_group('anger')\n",
        "sadness_male_data=male_emotion_groups.get_group('sadness')\n",
        "fear_male_data=male_emotion_groups.get_group('fear')\n",
        "joy_male_data=male_emotion_groups.get_group('joy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNTxGtK8iyK1"
      },
      "source": [
        "Collect data set for each emotion from the female dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTpYzRG-pzB3"
      },
      "outputs": [],
      "source": [
        "female_emotion_groups = female_data.groupby('Emotion')\n",
        "\n",
        "# Create separate datasets for male and female\n",
        "anger_female_data=female_emotion_groups.get_group('anger')\n",
        "sadness_female_data=female_emotion_groups.get_group('sadness')\n",
        "fear_female_data=female_emotion_groups.get_group('fear')\n",
        "joy_female_data=female_emotion_groups.get_group('joy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KyZLJdQpvvu"
      },
      "outputs": [],
      "source": [
        "print(anger_male_data.shape)\n",
        "print(sadness_male_data.shape)\n",
        "print(fear_male_data.shape)\n",
        "print(joy_male_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdyE8o2Nqrfe"
      },
      "outputs": [],
      "source": [
        "print(anger_female_data.shape)\n",
        "print(sadness_female_data.shape)\n",
        "print(fear_female_data.shape)\n",
        "print(joy_female_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFLAk-ya6GSz"
      },
      "source": [
        "The text in every index or row for each emotion was same for male and female and gender was different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1poGfjBsgJ4"
      },
      "outputs": [],
      "source": [
        "print(anger_male_data['Sentence'].iloc[900:902])\n",
        "print(anger_female_data['Sentence'].iloc[900:902])\n",
        "print(\"\\n\")\n",
        "print(fear_male_data['Sentence'].iloc[510:512])\n",
        "print(fear_female_data['Sentence'].iloc[510:512])\n",
        "print(\"\\n\")\n",
        "print(sadness_male_data['Sentence'].iloc[510:512])\n",
        "print(sadness_female_data['Sentence'].iloc[510:512])\n",
        "print(\"\\n\")\n",
        "print(joy_male_data['Sentence'].iloc[700:702])\n",
        "print(joy_female_data['Sentence'].iloc[700:702])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dai030Re5NE5"
      },
      "source": [
        "No Emotion word datasets for male and female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks2y7zMe5Lsw"
      },
      "outputs": [],
      "source": [
        "# data_null_emotion = data[data['Emotion'].isnull()]\n",
        "\n",
        "# data_null_emotion.shape\n",
        "\n",
        "# # Group the data by gender\n",
        "# No_emotion_gender_groups = data_null_emotion.groupby('Gender')\n",
        "\n",
        "# # Create separate datasets for male and female\n",
        "# No_emotion_male_data = No_emotion_gender_groups.get_group('male')\n",
        "# No_emotion_female_data = No_emotion_gender_groups.get_group('female')\n",
        "# print(No_emotion_female_data.shape)\n",
        "# print(No_emotion_male_data.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNZIvmqOkdoC"
      },
      "source": [
        "Null Race\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0FBmpbukJtl"
      },
      "outputs": [],
      "source": [
        "# data_null_Race = data[data['Race'].isnull()]\n",
        "# data_null_Race.shape\n",
        "# No_Race_Gender_group=data_null_Race.groupby('Gender')\n",
        "# No_race_male_data=No_Race_Gender_group.get_group('male')\n",
        "# No_race_female_data=No_Race_Gender_group.get_group('female')\n",
        "\n",
        "# print(No_race_female_data.shape)\n",
        "# print(No_race_male_data.shape)\n",
        "\n",
        "# print(No_race_female_data['Sentence'].iloc[100:101])\n",
        "# print(No_race_male_data['Sentence'].iloc[100:101])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw9uHG5at6rR"
      },
      "source": [
        "# **Using TfIdf Vectorizer**\n",
        "\n",
        "pipeline_tfidf_lr_best-- model name after hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wPDkSL17vbn"
      },
      "source": [
        "# Bias checking for anger emotion for male and female datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F0O6pwRE068"
      },
      "source": [
        "Anger Male\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr0vFGjrTVra"
      },
      "outputs": [],
      "source": [
        "class_names = pipeline_tfidf_lr_best.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-ba-SS08NtK"
      },
      "outputs": [],
      "source": [
        "new_anger_male=anger_male_data[['Sentence']]\n",
        "new_anger_male.shape\n",
        "anger_male_test = tfidf_vectorizer.transform(new_anger_male['Sentence'])\n",
        "anger_male_pred = pipeline_tfidf_lr_best.predict_proba(anger_male_test)\n",
        "anger_male_pred\n",
        "for i in range(len(anger_male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={anger_male_pred[i][0]}, {class_names[1]}={anger_male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNB1au10FACM"
      },
      "source": [
        "Anger Female "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz2tuzdeD12M"
      },
      "outputs": [],
      "source": [
        "new_anger_female=anger_female_data[['Sentence']]\n",
        "anger_female_test= tfidf_vectorizer.transform(new_anger_female['Sentence'])\n",
        "anger_female_pred= pipeline_tfidf_lr_best.predict_proba(anger_female_test)\n",
        "anger_female_pred\n",
        "for i in range(len(anger_female_pred)):\n",
        "     print(f\"Observation {i}: {class_names[0]}={anger_female_pred[i][0]}, {class_names[1]}={anger_female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSpG2-frlsyb"
      },
      "source": [
        "Checking the intensity average for anger emotion and checking bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bejXPPzjHFNE"
      },
      "outputs": [],
      "source": [
        "diff_anger = []\n",
        "diff_anger_count = 0  # number of pairs with differences\n",
        "same_anger_count = 0  # number of pairs without differences\n",
        "anger_male_prob_sum=0\n",
        "anger_female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(anger_female_pred)):\n",
        "          female_prob = anger_female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = anger_male_pred[i][1]  # probability of positive class for male sentence i\n",
        "          anger_male_prob_sum+=male_prob # male probabilities sum\n",
        "          anger_female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          diff_anger.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_anger_count += 1\n",
        "          else:\n",
        "              same_anger_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_anger_count > 0:\n",
        "    avg_diff = sum(diff_anger) / diff_anger_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_anger_count)\n",
        "print(\"Number of pairs without differences:\", same_anger_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_anger_male_scores=(anger_male_prob_sum)/len(anger_male_pred)\n",
        "avg_anger_female_scores=(anger_female_prob_sum)/len(anger_female_pred)\n",
        "print(\"Male anger Average\",avg_anger_male_scores)\n",
        "print(\"Female anger Average\",avg_anger_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_anger_female_scores > avg_anger_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXyHvehPLdvC"
      },
      "source": [
        "# Bias checking for sadness emotion for male and female datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crNl4F9vNdEQ"
      },
      "source": [
        "Sadness Male"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS4hlDpILbk9"
      },
      "outputs": [],
      "source": [
        "new_sadness_male=sadness_male_data[['Sentence']]\n",
        "new_sadness_male.shape\n",
        "sadness_male_test = tfidf_vectorizer.transform(new_sadness_male['Sentence'])\n",
        "sadness_male_pred = pipeline_tfidf_lr_best.predict_proba(sadness_male_test)\n",
        "sadness_male_pred\n",
        "for i in range(len(sadness_male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={sadness_male_pred[i][0]}, {class_names[1]}={sadness_male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNBKFmmLNfzO"
      },
      "source": [
        "Sadness Female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgzKQZWSLb16"
      },
      "outputs": [],
      "source": [
        "new_sadness_female=sadness_female_data[['Sentence']]\n",
        "sadness_female_test= tfidf_vectorizer.transform(new_sadness_female['Sentence'])\n",
        "sadness_female_pred= pipeline_tfidf_lr_best.predict_proba(sadness_female_test)\n",
        "sadness_female_pred\n",
        "for i in range(len(sadness_female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={sadness_female_pred[i][0]}, {class_names[1]}={sadness_female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNZGApiasDqu"
      },
      "source": [
        "Checking the intensity average for sadness emotion and checking bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNVaeCcfLb5y"
      },
      "outputs": [],
      "source": [
        "diff_sadness = []\n",
        "diff_sadness_count = 0  # number of pairs with differences\n",
        "same_sadness_count = 0  # number of pairs without differences\n",
        "sadness_male_prob_sum=0\n",
        "sadness_female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(sadness_female_pred)):\n",
        "          female_prob = sadness_female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = sadness_male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          sadness_male_prob_sum+=male_prob # male probabilities sum\n",
        "          sadness_female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diff_sadness.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_sadness_count += 1\n",
        "          else:\n",
        "              same_sadness_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_sadness_count > 0:\n",
        "    avg_diff = sum(diff_sadness) / diff_sadness_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_sadness_count)\n",
        "print(\"Number of pairs without differences:\", same_sadness_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_sadness_male_scores=(sadness_male_prob_sum)/len(sadness_male_pred)\n",
        "avg_sadness_female_scores=(sadness_female_prob_sum)/len(sadness_female_pred)\n",
        "print(\"Male sadness Average\",avg_sadness_male_scores)\n",
        "print(\"Female sadness Average\",avg_sadness_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_sadness_female_scores > avg_sadness_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpn1cMSYwQ4Y"
      },
      "source": [
        "# Bias checking for joy emotion for male and female datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iziGJd1zsjAF"
      },
      "source": [
        "Joy male"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrOfK2r6LcCb"
      },
      "outputs": [],
      "source": [
        "new_joy_male=joy_male_data[['Sentence']]\n",
        "new_joy_male.shape\n",
        "joy_male_test = tfidf_vectorizer.transform(new_joy_male['Sentence'])\n",
        "joy_male_pred = pipeline_tfidf_lr_best.predict_proba(joy_male_test)\n",
        "joy_male_pred\n",
        "for i in range(len(joy_male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={joy_male_pred[i][0]}, {class_names[1]}={joy_male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RgrSV3bsnQP"
      },
      "source": [
        "Joy female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk94UXbdLLNY"
      },
      "outputs": [],
      "source": [
        "new_joy_female=joy_female_data[['Sentence']]\n",
        "joy_female_test= tfidf_vectorizer.transform(new_joy_female['Sentence'])\n",
        "joy_female_pred= pipeline_tfidf_lr_best.predict_proba(joy_female_test)\n",
        "joy_female_pred\n",
        "for i in range(len(joy_female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={joy_female_pred[i][0]}, {class_names[1]}={joy_female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6MugO7TsssI"
      },
      "source": [
        "Checking the intensity average for joy emotion and checking bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwGVU6_gLLbg"
      },
      "outputs": [],
      "source": [
        "diff_joy = []\n",
        "diff_joy_count = 0  # number of pairs with differences\n",
        "same_joy_count = 0  # number of pairs without differences\n",
        "joy_male_prob_sum=0\n",
        "joy_female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(joy_female_pred)):\n",
        "          female_prob = joy_female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = joy_male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          joy_male_prob_sum+=male_prob # male probabilities sum\n",
        "          joy_female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diff_joy.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_joy_count += 1\n",
        "          else:\n",
        "              same_joy_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_joy_count > 0:\n",
        "    avg_diff = sum(diff_joy) / diff_joy_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_joy_count)\n",
        "print(\"Number of pairs without differences:\", same_joy_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_joy_male_scores=(joy_male_prob_sum)/len(joy_male_pred)\n",
        "avg_joy_female_scores=(joy_female_prob_sum)/len(joy_female_pred)\n",
        "print(\"Male joy Average\",avg_joy_male_scores)\n",
        "print(\"Female joy Average\",avg_joy_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_joy_female_scores > avg_joy_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdGAvXDSwYEO"
      },
      "source": [
        "# Bias checking for fear emotion for male and female datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UN76c8Ws_qq"
      },
      "source": [
        "fear male"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9ULT7J7wXEw"
      },
      "outputs": [],
      "source": [
        "new_fear_male=fear_male_data[['Sentence']]\n",
        "new_fear_male.shape\n",
        "fear_male_test = tfidf_vectorizer.transform(new_fear_male['Sentence'])\n",
        "fear_male_pred = pipeline_tfidf_lr_best.predict_proba(fear_male_test)\n",
        "fear_male_pred\n",
        "for i in range(len(fear_male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={fear_male_pred[i][0]}, {class_names[1]}={fear_male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBnpDG1HtDZt"
      },
      "source": [
        "Fear Feamle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCrvYARPOJnL"
      },
      "outputs": [],
      "source": [
        "new_fear_female=fear_female_data[['Sentence']]\n",
        "fear_female_test= tfidf_vectorizer.transform(new_fear_female['Sentence'])\n",
        "fear_female_pred= pipeline_tfidf_lr_best.predict_proba(fear_female_test)\n",
        "fear_female_pred\n",
        "for i in range(len(fear_female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={fear_female_pred[i][0]}, {class_names[1]}={fear_female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R-41_K1tGuM"
      },
      "source": [
        "Checking the intensity average for Fear emotion and checking bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMMhriu0OXLZ"
      },
      "outputs": [],
      "source": [
        "diff_fear = []\n",
        "diff_fear_count = 0  # number of pairs with differences\n",
        "same_fear_count = 0  # number of pairs without differences\n",
        "fear_male_prob_sum=0\n",
        "fear_female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(fear_female_pred)):\n",
        "          female_prob = fear_female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = fear_male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          fear_male_prob_sum+=male_prob # male probabilities sum\n",
        "          fear_female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diff_fear.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_fear_count += 1\n",
        "          else:\n",
        "              same_fear_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_joy_count > 0:\n",
        "    avg_diff = sum(diff_fear) / diff_fear_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_fear_count)\n",
        "print(\"Number of pairs without differences:\", same_fear_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_fear_male_scores=(fear_male_prob_sum)/len(fear_male_pred)\n",
        "avg_fear_female_scores=(fear_female_prob_sum)/len(fear_female_pred)\n",
        "print(\"Male fear Average\",avg_fear_male_scores)\n",
        "print(\"Female fear Average\",avg_fear_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_fear_female_scores > avg_fear_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUY3OGIDv8qN"
      },
      "source": [
        "# Checking the bias for entire male and female datsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRTJ1jLav6rB"
      },
      "source": [
        "pipeline_tfidf_lr_best-- model name after hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi1hJf98udcD"
      },
      "source": [
        "Male EEC corpus data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT94oTO4w-Jy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w9EfrkHQdYt"
      },
      "outputs": [],
      "source": [
        "new_male_df = male_data[['Sentence']]\n",
        "male_test = tfidf_vectorizer.transform(new_male_df['Sentence'])\n",
        "#male_pred = pipeline_tfidf_lr_best.predict(male_test)\n",
        "male_pred = pipeline_tfidf_lr_best.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names = pipeline_tfidf_lr_best.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nj1R-yevWZA"
      },
      "source": [
        "Female EEc Corpus data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTPkg0GnQdj4"
      },
      "outputs": [],
      "source": [
        "new_female_df = female_data[['Sentence']]\n",
        "female_test = tfidf_vectorizer.transform(new_female_df['Sentence'])\n",
        "female_pred = pipeline_tfidf_lr_best.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_tfidf_lr_best.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmXdXZSJupO1"
      },
      "source": [
        "Checking the intensity Average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKDdujrScDVY"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUyH4qmD_Br3"
      },
      "source": [
        "# Bias checking for Non-emotion datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0LzlkGxCCly"
      },
      "outputs": [],
      "source": [
        "data_null_emotion = data[data['Emotion'].isnull()]\n",
        "\n",
        "data_null_emotion.shape\n",
        "\n",
        "# Group the data by gender\n",
        "No_emotion_gender_groups = data_null_emotion.groupby('Gender')\n",
        "\n",
        "# Create separate datasets for male and female\n",
        "No_emotion_male_data = No_emotion_gender_groups.get_group('male')\n",
        "No_emotion_female_data = No_emotion_gender_groups.get_group('female')\n",
        "print(No_emotion_female_data.shape)\n",
        "print(No_emotion_male_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-05WBSRwtRY"
      },
      "outputs": [],
      "source": [
        "print(No_emotion_male_data['Sentence'].iloc[10:15])\n",
        "print(No_emotion_female_data['Sentence'].iloc[10:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmyvl8AjXX8W"
      },
      "outputs": [],
      "source": [
        "male_test = tfidf_vectorizer.transform(No_emotion_male_data['Sentence'])\n",
        "male_pred = pipeline_tfidf_lr_best.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names = pipeline_tfidf_lr_best.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7padeaNi8gla"
      },
      "outputs": [],
      "source": [
        "female_test = tfidf_vectorizer.transform(No_emotion_female_data['Sentence'])\n",
        "female_pred = pipeline_tfidf_lr_best.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_tfidf_lr_best.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM7idMiz83aE"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY7il-7SzRHf"
      },
      "source": [
        "# **Race** Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp2RpVFPXZMd"
      },
      "outputs": [],
      "source": [
        "print(data['Race'].value_counts())\n",
        "# Group the data by Race\n",
        "Race_groups = data.groupby('Race')\n",
        "\n",
        "# Create separate datasets for African-American and European\n",
        "AfA_data = Race_groups.get_group('African-American')\n",
        "Euro_data = Race_groups.get_group('European')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW32Ysk7XgNF"
      },
      "outputs": [],
      "source": [
        "print(AfA_data.shape)\n",
        "print(Euro_data.shape)\n",
        "\n",
        "print(AfA_data['Emotion'].value_counts())\n",
        "print(Euro_data['Emotion'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k24k8dpwXZeV"
      },
      "outputs": [],
      "source": [
        "print(AfA_data['Sentence'].iloc[100:109])\n",
        "print(Euro_data['Sentence'].iloc[100:109])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKLfE7AJwOPz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTBH9hVPbStm"
      },
      "source": [
        "# Bias for Entire Race DataSets(AA and Eu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV_8JOErX1ln"
      },
      "source": [
        "African-American DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSzivWumRGHT"
      },
      "outputs": [],
      "source": [
        "AfA_test = tfidf_vectorizer.transform(AfA_data['Sentence'])\n",
        "AfA_pred = pipeline_tfidf_lr_best.predict_proba(AfA_test)\n",
        "AfA_pred\n",
        "class_names = pipeline_tfidf_lr_best.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={AfA_pred[i][0]}, {class_names[1]}={AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW0StF1mXvZb"
      },
      "source": [
        "Euro Data Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXhCo4HrQd2u"
      },
      "outputs": [],
      "source": [
        "Euro_test=tfidf_vectorizer.transform(Euro_data['Sentence'])\n",
        "Euro_pred=pipeline_tfidf_lr_best.predict_proba(Euro_test)\n",
        "Euro_pred\n",
        "for i in range(len(Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={Euro_pred[i][0]}, {class_names[1]}={Euro_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdu7bcjAzkac"
      },
      "outputs": [],
      "source": [
        "diff_list = []\n",
        "diff_count_race = 0  # number of pairs with differences\n",
        "same_count_race = 0  # number of pairs without differences\n",
        "Euro_prob_sum=0\n",
        "AfA_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(Euro_pred)):\n",
        "          Euro_prob = Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = AfA_pred[i][1]  # probability of positive class for AfA sentence i\n",
        "          Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_list.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count_race += 1\n",
        "          else:\n",
        "              same_count_race += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count_race > 0:\n",
        "    avg_diff_race = sum(diff_list) / diff_count_race\n",
        "else:\n",
        "    avg_diff_race = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count_race)\n",
        "print(\"Number of pairs without differences:\", same_count_race)\n",
        "print(\"Average difference:\", avg_diff_race)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw1svldFzkyK"
      },
      "outputs": [],
      "source": [
        "avg_AfA_scores=(AfA_prob_sum)/len(AfA_pred)\n",
        "avg_Euro_scores=(Euro_prob_sum)/len(Euro_pred)\n",
        "\n",
        "print(avg_AfA_scores)\n",
        "print(avg_Euro_scores)\n",
        "\n",
        "# Check if E=A not significant\n",
        "if abs(avg_diff_race) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_Euro_scores > avg_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s0r4Tej0fw4"
      },
      "source": [
        "Bias Measure for Emtions in Race DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxvSlOoQzk1D"
      },
      "outputs": [],
      "source": [
        "AfA_emotion_groups = AfA_data.groupby('Emotion')\n",
        "\n",
        "# Create separate datasets for AfA and Euro\n",
        "anger_AfA_data = AfA_emotion_groups.get_group('anger')\n",
        "sadness_AfA_data=AfA_emotion_groups.get_group('sadness')\n",
        "fear_AfA_data=AfA_emotion_groups.get_group('fear')\n",
        "joy_AfA_data=AfA_emotion_groups.get_group('joy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShSvIQGSykjP"
      },
      "outputs": [],
      "source": [
        "Euro_emotion_groups = Euro_data.groupby('Emotion')\n",
        "\n",
        "# Create separate datasets for AfA and Euro\n",
        "anger_Euro_data = Euro_emotion_groups.get_group('anger')\n",
        "sadness_Euro_data=Euro_emotion_groups.get_group('sadness')\n",
        "fear_Euro_data=Euro_emotion_groups.get_group('fear')\n",
        "joy_Euro_data=Euro_emotion_groups.get_group('joy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPIMFAtUykl-"
      },
      "outputs": [],
      "source": [
        "print(anger_AfA_data.shape)\n",
        "print(anger_Euro_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3KOoNALoL1R"
      },
      "outputs": [],
      "source": [
        "print(sadness_AfA_data.shape)\n",
        "print(sadness_Euro_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZjMOE0koMBj"
      },
      "outputs": [],
      "source": [
        "print(fear_AfA_data.shape)\n",
        "print(fear_Euro_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m2i2-DnoMNu"
      },
      "outputs": [],
      "source": [
        "print(joy_AfA_data.shape)\n",
        "print(joy_Euro_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCCY-RuU1na4"
      },
      "source": [
        "The text in AfA and Euro data sets are same for every index or row, but Race was different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpWrPHk71P3W"
      },
      "outputs": [],
      "source": [
        "print(anger_AfA_data['Sentence'].iloc[305:310])\n",
        "print(anger_Euro_data['Sentence'].iloc[305:310])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l551MWjY8eh_"
      },
      "source": [
        "# **Bias for Anger emotion for Race**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZLZEwAvogXY"
      },
      "source": [
        "Anger AfA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnHbLZ0q8c3A"
      },
      "outputs": [],
      "source": [
        "new_anger_AfA=anger_AfA_data[['Sentence']]\n",
        "new_anger_AfA.shape\n",
        "anger_AfA_test = tfidf_vectorizer.transform(new_anger_AfA['Sentence'])\n",
        "anger_AfA_pred = pipeline_tfidf_lr_best.predict_proba(anger_AfA_test)\n",
        "anger_AfA_pred\n",
        "for i in range(len(anger_AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={anger_AfA_pred[i][0]}, {class_names[1]}={anger_AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pFQ8uyEomoL"
      },
      "source": [
        "Anger Euro DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj_hiFnG8dK-"
      },
      "outputs": [],
      "source": [
        "new_anger_Euro=anger_Euro_data[['Sentence']]\n",
        "anger_Euro_test= tfidf_vectorizer.transform(new_anger_Euro['Sentence'])\n",
        "anger_Euro_pred= pipeline_tfidf_lr_best.predict_proba(anger_Euro_test)\n",
        "anger_Euro_pred\n",
        "for i in range(len(anger_Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={anger_Euro_pred[i][0]}, {class_names[1]}={anger_Euro_pred[i][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKrX0Zpcoydc"
      },
      "source": [
        "Checking intensity score average and Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BziWZlfo8dWd"
      },
      "outputs": [],
      "source": [
        "diff_anger = []\n",
        "diff_anger_count = 0  # number of pairs with differences\n",
        "same_anger_count = 0  # number of pairs without differences\n",
        "anger_AfA_prob_sum=0\n",
        "anger_Euro_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(anger_Euro_pred)):\n",
        "          Euro_prob = anger_Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = anger_AfA_pred[i][1]    # probability of positive class for AfA sentence i\n",
        "          anger_AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          anger_Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_anger.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_anger_count += 1\n",
        "          else:\n",
        "              same_anger_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_anger_count > 0:\n",
        "    avg_diff = sum(diff_anger) / diff_anger_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_anger_count)\n",
        "print(\"Number of pairs without differences:\", same_anger_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_anger_AfA_scores=(anger_AfA_prob_sum)/len(anger_AfA_pred)\n",
        "avg_anger_Euro_scores=(anger_Euro_prob_sum)/len(anger_Euro_pred)\n",
        "print(\"AfA anger Average\",avg_anger_AfA_scores)\n",
        "print(\"Euro anger Average\",avg_anger_Euro_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_anger_Euro_scores > avg_anger_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "# Check if E↓–A↑ significant\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmZNLMUjWfjU"
      },
      "source": [
        "# **Sadness Bias for Race**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiuld4Slpa1G"
      },
      "source": [
        "sadness AfA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXxK5mKAQ7F-"
      },
      "outputs": [],
      "source": [
        "new_sadness_AfA=sadness_AfA_data[['Sentence']]\n",
        "new_sadness_AfA.shape\n",
        "sadness_AfA_test = tfidf_vectorizer.transform(new_sadness_AfA['Sentence'])\n",
        "sadness_AfA_pred = pipeline_tfidf_lr_best.predict_proba(sadness_AfA_test)\n",
        "sadness_AfA_pred\n",
        "for i in range(len(sadness_AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={sadness_AfA_pred[i][0]}, {class_names[1]}={sadness_AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6yotvjOphMa"
      },
      "source": [
        "Sadness Euro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXeMpAU4Q7Vt"
      },
      "outputs": [],
      "source": [
        "new_sadness_Euro=sadness_Euro_data[['Sentence']]\n",
        "sadness_Euro_test= tfidf_vectorizer.transform(new_sadness_Euro['Sentence'])\n",
        "sadness_Euro_pred= pipeline_tfidf_lr_best.predict_proba(sadness_Euro_test)\n",
        "sadness_Euro_pred\n",
        "for i in range(len(sadness_Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={sadness_Euro_pred[i][0]}, {class_names[1]}={sadness_Euro_pred[i][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj0SYkjopnDY"
      },
      "source": [
        "Checking intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhKK9QUlQ7ht"
      },
      "outputs": [],
      "source": [
        "diff_sadness = []\n",
        "diff_sadness_count = 0  # number of pairs with differences\n",
        "same_sadness_count = 0  # number of pairs without differences\n",
        "sadness_AfA_prob_sum=0\n",
        "sadness_Euro_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(sadness_Euro_pred)):\n",
        "          Euro_prob = sadness_Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = sadness_AfA_pred[i][1]    # probability of positive class for AfA sentence i\n",
        "          sadness_AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          sadness_Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_sadness.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_sadness_count += 1\n",
        "          else:\n",
        "              same_sadness_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_sadness_count > 0:\n",
        "    avg_diff = sum(diff_sadness) / diff_sadness_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_sadness_count)\n",
        "print(\"Number of pairs without differences:\", same_sadness_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_sadness_AfA_scores=(sadness_AfA_prob_sum)/len(sadness_AfA_pred)\n",
        "avg_sadness_Euro_scores=(sadness_Euro_prob_sum)/len(sadness_Euro_pred)\n",
        "print(\"AfA sadness Average\",avg_sadness_AfA_scores)\n",
        "print(\"Euro sadness Average\",avg_sadness_Euro_scores)\n",
        "\n",
        "# Check if E=A not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_sadness_Euro_scores > avg_sadness_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "# Check if E↓–A↑ significant\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKYg56UHY3kh"
      },
      "source": [
        "# **Joy Bias for Race**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-St6sEeqEsb"
      },
      "source": [
        "joy AfA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1cYwT0fY1tE"
      },
      "outputs": [],
      "source": [
        "new_joy_AfA=joy_AfA_data[['Sentence']]\n",
        "new_joy_AfA.shape\n",
        "joy_AfA_test = tfidf_vectorizer.transform(new_joy_AfA['Sentence'])\n",
        "joy_AfA_pred = pipeline_tfidf_lr_best.predict_proba(joy_AfA_test)\n",
        "joy_AfA_pred\n",
        "for i in range(len(joy_AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={joy_AfA_pred[i][0]}, {class_names[1]}={joy_AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaP6YGMSqKiV"
      },
      "source": [
        "joy Euro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40TzzRjIY14y"
      },
      "outputs": [],
      "source": [
        "new_joy_Euro=joy_Euro_data[['Sentence']]\n",
        "joy_Euro_test= tfidf_vectorizer.transform(new_joy_Euro['Sentence'])\n",
        "joy_Euro_pred= pipeline_tfidf_lr_best.predict_proba(joy_Euro_test)\n",
        "joy_Euro_pred\n",
        "for i in range(len(joy_Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={joy_Euro_pred[i][0]}, {class_names[1]}={joy_Euro_pred[i][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQpLSuLCqQxZ"
      },
      "source": [
        "Checking intensity score average and Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00k_4h8aY2EC"
      },
      "outputs": [],
      "source": [
        "diff_joy = []\n",
        "diff_joy_count = 0  # number of pairs with differences\n",
        "same_joy_count = 0  # number of pairs without differences\n",
        "joy_AfA_prob_sum=0\n",
        "joy_Euro_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(joy_Euro_pred)):\n",
        "          Euro_prob = joy_Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = joy_AfA_pred[i][1]    # probability of positive class for AfA sentence i\n",
        "          joy_AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          joy_Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_joy.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_joy_count += 1\n",
        "          else:\n",
        "              same_joy_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_joy_count > 0:\n",
        "    avg_diff = sum(diff_joy) / diff_joy_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_joy_count)\n",
        "print(\"Number of pairs without differences:\", same_joy_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_joy_AfA_scores=(joy_AfA_prob_sum)/len(joy_AfA_pred)\n",
        "avg_joy_Euro_scores=(joy_Euro_prob_sum)/len(joy_Euro_pred)\n",
        "print(\"AfA joy Average\",avg_joy_AfA_scores)\n",
        "print(\"Euro joy Average\",avg_joy_Euro_scores)\n",
        "\n",
        "# Check if E=A not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_joy_Euro_scores > avg_joy_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "# Check if E↓–A↑ significant\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8syNf9jI4QTM"
      },
      "source": [
        "# Bias for fear emotion "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHdJeIn1o8nK"
      },
      "source": [
        "Fear AfA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n53oiY8rq5X7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1RroJbT1QBK"
      },
      "outputs": [],
      "source": [
        "new_fear_AfA=fear_AfA_data[['Sentence']]\n",
        "new_fear_AfA.shape\n",
        "fear_AfA_test = tfidf_vectorizer.transform(new_fear_AfA['Sentence'])\n",
        "fear_AfA_pred = pipeline_tfidf_lr_best.predict_proba(fear_AfA_test)\n",
        "fear_AfA_pred\n",
        "for i in range(len(fear_AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={fear_AfA_pred[i][0]}, {class_names[1]}={fear_AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iMyakuDpBPR"
      },
      "source": [
        "Fear Euro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uaSHi70rDmS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idF7CF0N1QGk"
      },
      "outputs": [],
      "source": [
        "new_fear_Euro=fear_Euro_data[['Sentence']]\n",
        "fear_Euro_test= tfidf_vectorizer.transform(new_fear_Euro['Sentence'])\n",
        "fear_Euro_pred= pipeline_tfidf_lr_best.predict_proba(fear_Euro_test)\n",
        "fear_Euro_pred\n",
        "for i in range(len(fear_Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={fear_Euro_pred[i][0]}, {class_names[1]}={fear_Euro_pred[i][1]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaAq0AZ2pFiz"
      },
      "source": [
        "Checking intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKL3KEMn1QJt"
      },
      "outputs": [],
      "source": [
        "diff_fear = []\n",
        "diff_fear_count = 0  # number of pairs with differences\n",
        "same_fear_count = 0  # number of pairs without differences\n",
        "fear_AfA_prob_sum=0\n",
        "fear_Euro_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(fear_Euro_pred)):\n",
        "          Euro_prob = fear_Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = fear_AfA_pred[i][1]    # probability of positive class for AfA sentence i\n",
        "          fear_AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          fear_Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_fear.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_fear_count += 1\n",
        "          else:\n",
        "              same_fear_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_fear_count > 0:\n",
        "    avg_diff = sum(diff_fear) / diff_fear_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_fear_count)\n",
        "print(\"Number of pairs without differences:\", same_fear_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_fear_AfA_scores=(fear_AfA_prob_sum)/len(fear_AfA_pred)\n",
        "avg_fear_Euro_scores=(fear_Euro_prob_sum)/len(fear_Euro_pred)\n",
        "print(\"AfA fear Average\",avg_fear_AfA_scores)\n",
        "print(\"Euro fear Average\",avg_fear_Euro_scores)\n",
        "\n",
        "# Check if E=A not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_fear_Euro_scores > avg_fear_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "# Check if E↓–A↑ significant\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bThsdXEr2yKD"
      },
      "source": [
        "# Non- Race and Group by Gender "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slfEOm9t2wRN"
      },
      "outputs": [],
      "source": [
        "data_null_Race = data[data['Race'].isnull()]\n",
        "data_null_Race.shape\n",
        "No_Race_Gender_group=data_null_Race.groupby('Gender')\n",
        "No_race_male_data=No_Race_Gender_group.get_group('male')\n",
        "No_race_female_data=No_Race_Gender_group.get_group('female')\n",
        "\n",
        "print(No_race_female_data.shape)\n",
        "print(No_race_male_data.shape)\n",
        "\n",
        "print(No_race_female_data['Sentence'].iloc[100:101])\n",
        "print(No_race_male_data['Sentence'].iloc[100:101])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QAd4zUO2wU6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGxwPKTk4Lmt"
      },
      "outputs": [],
      "source": [
        "male_test = tfidf_vectorizer.transform(No_race_male_data['Sentence'])\n",
        "male_pred = pipeline_tfidf_lr_best.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names = pipeline_tfidf_lr_best.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnM3pWuA4Lmv"
      },
      "outputs": [],
      "source": [
        "female_test = tfidf_vectorizer.transform(No_race_female_data['Sentence'])\n",
        "female_pred = pipeline_tfidf_lr_best.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_tfidf_lr_best.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTIHtpOO4Lmw"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQfNHaXR2wnW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARYsDFrNb_Yr"
      },
      "source": [
        "# **CountVectorizer**\n",
        "\n",
        "pipeline_count --- the model after hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGoqo0qKd8it"
      },
      "source": [
        "**Gender Bias**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icKuhsc3rfdI"
      },
      "source": [
        "male dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZgSaV-Tcp60"
      },
      "outputs": [],
      "source": [
        "new_male_df = male_data[['Sentence']]\n",
        "male_test = count_vectorizer.transform(new_male_df['Sentence'])\n",
        "#male_pred = pipeline_count.predict(male_test)\n",
        "male_pred = pipeline_count.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names =pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z02qGxfcrlH4"
      },
      "source": [
        "Female Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-thIAJ1ncp61"
      },
      "outputs": [],
      "source": [
        "new_female_df = female_data[['Sentence']]\n",
        "female_test = count_vectorizer.transform(new_female_df['Sentence'])\n",
        "female_pred = pipeline_count.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uXgPIPUrqH9"
      },
      "source": [
        "Checking intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpu3y0Gfcp61"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psECaDeXb-oC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kafHO7mxmjH5"
      },
      "source": [
        "# Anger dataset Bias for male and female"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB56djWyr6AH"
      },
      "source": [
        "Anger male dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAmVBmu4mcXo"
      },
      "outputs": [],
      "source": [
        "new_anger_male=anger_male_data[['Sentence']]\n",
        "new_anger_male.shape\n",
        "anger_male_test = count_vectorizer.transform(new_anger_male['Sentence'])\n",
        "anger_male_pred = pipeline_count.predict_proba(anger_male_test)\n",
        "anger_male_pred\n",
        "for i in range(len(anger_male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={anger_male_pred[i][0]}, {class_names[1]}={anger_male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz-thsKGr_q_"
      },
      "source": [
        "Anger female dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxBrZJ2WmcXo"
      },
      "outputs": [],
      "source": [
        "new_anger_female=anger_female_data[['Sentence']]\n",
        "anger_female_test= count_vectorizer.transform(new_anger_female['Sentence'])\n",
        "anger_female_pred= pipeline_count.predict_proba(anger_female_test)\n",
        "anger_female_pred\n",
        "for i in range(len(anger_female_pred)):\n",
        "     print(f\"Observation {i}: {class_names[0]}={anger_female_pred[i][0]}, {class_names[1]}={anger_female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIw6cKl6sD4_"
      },
      "source": [
        "Checking intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0bcgeY7mcXp"
      },
      "outputs": [],
      "source": [
        "diff_anger = []\n",
        "diff_anger_count = 0  # number of pairs with differences\n",
        "same_anger_count = 0  # number of pairs without differences\n",
        "anger_male_prob_sum=0\n",
        "anger_female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(anger_female_pred)):\n",
        "          female_prob = anger_female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = anger_male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          anger_male_prob_sum+=male_prob # male probabilities sum\n",
        "          anger_female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diff_anger.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_anger_count += 1\n",
        "          else:\n",
        "              same_anger_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_anger_count > 0:\n",
        "    avg_diff = sum(diff_anger) / diff_anger_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_anger_count)\n",
        "print(\"Number of pairs without differences:\", same_anger_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_anger_male_scores=(anger_male_prob_sum)/len(anger_male_pred)\n",
        "avg_anger_female_scores=(anger_female_prob_sum)/len(anger_female_pred)\n",
        "print(\"Male anger Average\",avg_anger_male_scores)\n",
        "print(\"Female anger Average\",avg_anger_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_anger_female_scores > avg_anger_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v56u3nsCn8X_"
      },
      "source": [
        "# Sadness Bias for male and feamle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-0HjErUslqY"
      },
      "source": [
        "male sadness dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug6Qo-YImcXp"
      },
      "outputs": [],
      "source": [
        "new_sadness_male=sadness_male_data[['Sentence']]\n",
        "new_sadness_male.shape\n",
        "sadness_male_test = count_vectorizer.transform(new_sadness_male['Sentence'])\n",
        "sadness_male_pred = pipeline_count.predict_proba(sadness_male_test)\n",
        "sadness_male_pred\n",
        "for i in range(len(sadness_male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={sadness_male_pred[i][0]}, {class_names[1]}={sadness_male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxPpP8tPsqqH"
      },
      "source": [
        "female sandness dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjcS6Jz7mcXq"
      },
      "outputs": [],
      "source": [
        "new_sadness_female=sadness_female_data[['Sentence']]\n",
        "sadness_female_test= count_vectorizer.transform(new_sadness_female['Sentence'])\n",
        "sadness_female_pred= pipeline_count.predict_proba(sadness_female_test)\n",
        "sadness_female_pred\n",
        "for i in range(len(sadness_female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={sadness_female_pred[i][0]}, {class_names[1]}={sadness_female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5KVLpPHsuaQ"
      },
      "source": [
        "Checking the intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGtQh6eRmcXq"
      },
      "outputs": [],
      "source": [
        "diff_sadness = []\n",
        "diff_sadness_count = 0  # number of pairs with differences\n",
        "same_sadness_count = 0  # number of pairs without differences\n",
        "sadness_male_prob_sum=0\n",
        "sadness_female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(sadness_female_pred)):\n",
        "          female_prob = sadness_female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = sadness_male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          sadness_male_prob_sum+=male_prob # male probabilities sum\n",
        "          sadness_female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diff_sadness.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_sadness_count += 1\n",
        "          else:\n",
        "              same_sadness_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_sadness_count > 0:\n",
        "    avg_diff = sum(diff_sadness) / diff_sadness_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_sadness_count)\n",
        "print(\"Number of pairs without differences:\", same_sadness_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_sadness_male_scores=(sadness_male_prob_sum)/len(sadness_male_pred)\n",
        "avg_sadness_female_scores=(sadness_female_prob_sum)/len(sadness_female_pred)\n",
        "print(\"Male sadness Average\",avg_sadness_male_scores)\n",
        "print(\"Female sadness Average\",avg_sadness_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_sadness_female_scores > avg_sadness_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pgNU41ioSL9"
      },
      "source": [
        "# Joy Bias for male and female"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unSxOWghs-7H"
      },
      "source": [
        "joy male dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cotG1QjbmcXq"
      },
      "outputs": [],
      "source": [
        "new_joy_male=joy_male_data[['Sentence']]\n",
        "new_joy_male.shape\n",
        "joy_male_test = count_vectorizer.transform(new_joy_male['Sentence'])\n",
        "joy_male_pred = pipeline_count.predict_proba(joy_male_test)\n",
        "joy_male_pred\n",
        "for i in range(len(joy_male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={joy_male_pred[i][0]}, {class_names[1]}={joy_male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZL1fSyYtEud"
      },
      "source": [
        "Joy female dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uvjI8akmcXr"
      },
      "outputs": [],
      "source": [
        "new_joy_female=joy_female_data[['Sentence']]\n",
        "joy_female_test= count_vectorizer.transform(new_joy_female['Sentence'])\n",
        "joy_female_pred= pipeline_count.predict_proba(joy_female_test)\n",
        "joy_female_pred\n",
        "for i in range(len(joy_female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={joy_female_pred[i][0]}, {class_names[1]}={joy_female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITC5R5ZatLiu"
      },
      "source": [
        "Checking the intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2xYlAb_mcXr"
      },
      "outputs": [],
      "source": [
        "diff_joy = []\n",
        "diff_joy_count = 0  # number of pairs with differences\n",
        "same_joy_count = 0  # number of pairs without differences\n",
        "joy_male_prob_sum=0\n",
        "joy_female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(joy_female_pred)):\n",
        "          female_prob = joy_female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = joy_male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          joy_male_prob_sum+=male_prob # male probabilities sum\n",
        "          joy_female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diff_joy.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_joy_count += 1\n",
        "          else:\n",
        "              same_joy_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_joy_count > 0:\n",
        "    avg_diff = sum(diff_joy) / diff_joy_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_joy_count)\n",
        "print(\"Number of pairs without differences:\", same_joy_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_joy_male_scores=(joy_male_prob_sum)/len(joy_male_pred)\n",
        "avg_joy_female_scores=(joy_female_prob_sum)/len(joy_female_pred)\n",
        "print(\"Male joy Average\",avg_joy_male_scores)\n",
        "print(\"Female joy Average\",avg_joy_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_joy_female_scores > avg_joy_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNYZl9mXo5eO"
      },
      "source": [
        "# Fear Bias for male and female"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RotiUS2cttZ_"
      },
      "source": [
        "fear male dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlHqquC6mcXs"
      },
      "outputs": [],
      "source": [
        "new_fear_male=fear_male_data[['Sentence']]\n",
        "new_fear_male.shape\n",
        "fear_male_test = count_vectorizer.transform(new_fear_male['Sentence'])\n",
        "fear_male_pred = pipeline_count.predict_proba(fear_male_test)\n",
        "fear_male_pred\n",
        "for i in range(len(fear_male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={fear_male_pred[i][0]}, {class_names[1]}={fear_male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNpe84dctyLK"
      },
      "source": [
        "fear female dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lXpjfyomcXt"
      },
      "outputs": [],
      "source": [
        "new_fear_female=fear_female_data[['Sentence']]\n",
        "fear_female_test= count_vectorizer.transform(new_fear_female['Sentence'])\n",
        "fear_female_pred= pipeline_count.predict_proba(fear_female_test)\n",
        "fear_female_pred\n",
        "for i in range(len(fear_female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={fear_female_pred[i][0]}, {class_names[1]}={fear_female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LDb6vojt2oQ"
      },
      "source": [
        "Checking the intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io_JPKGLmcXt"
      },
      "outputs": [],
      "source": [
        "diff_fear = []\n",
        "diff_fear_count = 0  # number of pairs with differences\n",
        "same_fear_count = 0  # number of pairs without differences\n",
        "fear_male_prob_sum=0\n",
        "fear_female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(fear_female_pred)):\n",
        "          female_prob = fear_female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = fear_male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          fear_male_prob_sum+=male_prob # male probabilities sum\n",
        "          fear_female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          #print(f\"Observation {i+1}: {new_male_df[i+1]}, {new_female_df[i+1]}, {diff}\")\n",
        "          diff_fear.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_fear_count += 1\n",
        "          else:\n",
        "              same_fear_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_joy_count > 0:\n",
        "    avg_diff = sum(diff_fear) / diff_fear_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_fear_count)\n",
        "print(\"Number of pairs without differences:\", same_fear_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_fear_male_scores=(fear_male_prob_sum)/len(fear_male_pred)\n",
        "avg_fear_female_scores=(fear_female_prob_sum)/len(fear_female_pred)\n",
        "print(\"Male fear Average\",avg_fear_male_scores)\n",
        "print(\"Female fear Average\",avg_fear_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_fear_female_scores > avg_fear_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roNL49xWbqBc"
      },
      "source": [
        "# Non- Emotion Male and Female datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8P725tMb-w7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUIE8en5cGEb"
      },
      "outputs": [],
      "source": [
        "# data_null_emotion = data[data['Emotion'].isnull()]\n",
        "\n",
        "# data_null_emotion.shape\n",
        "\n",
        "# # Group the data by gender\n",
        "# No_emotion_gender_groups = data_null_emotion.groupby('Gender')\n",
        "\n",
        "# # Create separate datasets for male and female\n",
        "# No_emotion_male_data = No_emotion_gender_groups.get_group('male')\n",
        "# No_emotion_female_data = No_emotion_gender_groups.get_group('female')\n",
        "# print(No_emotion_female_data.shape)\n",
        "# print(No_emotion_male_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uh_QRtycGEc"
      },
      "outputs": [],
      "source": [
        "male_test = count_vectorizer.transform(No_emotion_male_data['Sentence'])\n",
        "male_pred =pipeline_count.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kld7qKptcGEd"
      },
      "outputs": [],
      "source": [
        "female_test = count_vectorizer.transform(No_emotion_female_data['Sentence'])\n",
        "female_pred = pipeline_count.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku8MzJyRcGEd"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5fToYlLeqWH"
      },
      "source": [
        "# **Race Bias using CountVectorizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOnluAzHvRIy"
      },
      "source": [
        "AfA Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aoM9VIPe9Hz"
      },
      "outputs": [],
      "source": [
        "Euro_data['Emotion'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS1_ZQtGeoww"
      },
      "outputs": [],
      "source": [
        "AfA_test = count_vectorizer.transform(AfA_data['Sentence'])\n",
        "AfA_pred = pipeline_count.predict_proba(AfA_test)\n",
        "AfA_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={AfA_pred[i][0]}, {class_names[1]}={AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0h52mr_vWY_"
      },
      "source": [
        "Euro Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3Akah6Ueoww"
      },
      "outputs": [],
      "source": [
        "Euro_test=count_vectorizer.transform(Euro_data['Sentence'])\n",
        "Euro_pred=pipeline_count.predict_proba(Euro_test)\n",
        "Euro_pred\n",
        "for i in range(len(Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={Euro_pred[i][0]}, {class_names[1]}={Euro_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw007O5lvadW"
      },
      "source": [
        "Checking the intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfLkPxAVeoww"
      },
      "outputs": [],
      "source": [
        "diff_list = []\n",
        "diff_count_race = 0  # number of pairs with differences\n",
        "same_count_race = 0  # number of pairs without differences\n",
        "Euro_prob_sum=0\n",
        "AfA_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(Euro_pred)):\n",
        "          Euro_prob = Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = AfA_pred[i][1]  # probability of positive class for AfA sentence i\n",
        "          Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_list.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count_race += 1\n",
        "          else:\n",
        "              same_count_race += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count_race > 0:\n",
        "    avg_diff_race = sum(diff_list) / diff_count_race\n",
        "else:\n",
        "    avg_diff_race = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count_race)\n",
        "print(\"Number of pairs without differences:\", same_count_race)\n",
        "print(\"Average difference:\", avg_diff_race)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4sVFTBXfXEv"
      },
      "outputs": [],
      "source": [
        "avg_AfA_scores=(AfA_prob_sum)/len(AfA_pred)\n",
        "avg_Euro_scores=(Euro_prob_sum)/len(Euro_pred)\n",
        "\n",
        "print(avg_AfA_scores)\n",
        "print(avg_Euro_scores)\n",
        "\n",
        "# Check if E=A not significant\n",
        "if abs(avg_diff_race) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_Euro_scores > avg_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcZYRaHxpvsl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSKJAHAMr6wF"
      },
      "source": [
        "**Race**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGwxxyaFH4Lg"
      },
      "source": [
        "# Anger Bias for Race"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2PNI2uquD9Y"
      },
      "source": [
        "Anger AfA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-6RFaJir2xc"
      },
      "outputs": [],
      "source": [
        "new_anger_AfA=anger_AfA_data[['Sentence']]\n",
        "new_anger_AfA.shape\n",
        "anger_AfA_test = count_vectorizer.transform(new_anger_AfA['Sentence'])\n",
        "anger_AfA_pred = pipeline_count.predict_proba(anger_AfA_test)\n",
        "anger_AfA_pred\n",
        "for i in range(len(anger_AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={anger_AfA_pred[i][0]}, {class_names[1]}={anger_AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKFj4s39vrmq"
      },
      "source": [
        "Anger Euro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCl1kxxir2xd"
      },
      "outputs": [],
      "source": [
        "new_anger_Euro=anger_Euro_data[['Sentence']]\n",
        "anger_Euro_test= count_vectorizer.transform(new_anger_Euro['Sentence'])\n",
        "anger_Euro_pred= pipeline_count.predict_proba(anger_Euro_test)\n",
        "anger_Euro_pred\n",
        "for i in range(len(anger_Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={anger_Euro_pred[i][0]}, {class_names[1]}={anger_Euro_pred[i][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3U2WRYMvvO8"
      },
      "source": [
        "Checking the intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB_1vcPFr2xe"
      },
      "outputs": [],
      "source": [
        "diff_anger = []\n",
        "diff_anger_count = 0  # number of pairs with differences\n",
        "same_anger_count = 0  # number of pairs without differences\n",
        "anger_AfA_prob_sum=0\n",
        "anger_Euro_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(anger_Euro_pred)):\n",
        "          Euro_prob = anger_Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = anger_AfA_pred[i][1]    # probability of positive class for AfA sentence i\n",
        "          anger_AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          anger_Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_anger.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_anger_count += 1\n",
        "          else:\n",
        "              same_anger_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_anger_count > 0:\n",
        "    avg_diff = sum(diff_anger) / diff_anger_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_anger_count)\n",
        "print(\"Number of pairs without differences:\", same_anger_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_anger_AfA_scores=(anger_AfA_prob_sum)/len(anger_AfA_pred)\n",
        "avg_anger_Euro_scores=(anger_Euro_prob_sum)/len(anger_Euro_pred)\n",
        "print(\"AfA anger Average\",avg_anger_AfA_scores)\n",
        "print(\"Euro anger Average\",avg_anger_Euro_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_anger_Euro_scores > avg_anger_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "# Check if E↓–A↑ significant\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FKph9Wat5JO"
      },
      "source": [
        "# Fear Bias for Race"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLSHgCAEv31u"
      },
      "source": [
        "fear AfA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec9jhSLGr2xf"
      },
      "outputs": [],
      "source": [
        "new_fear_AfA=fear_AfA_data[['Sentence']]\n",
        "new_fear_AfA.shape\n",
        "fear_AfA_test = count_vectorizer.transform(new_fear_AfA['Sentence'])\n",
        "fear_AfA_pred = pipeline_count.predict_proba(fear_AfA_test)\n",
        "fear_AfA_pred\n",
        "for i in range(len(fear_AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={fear_AfA_pred[i][0]}, {class_names[1]}={fear_AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mpHmDO7wBxV"
      },
      "source": [
        "Fear Euro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXKjYYB9r2xg"
      },
      "outputs": [],
      "source": [
        "new_fear_Euro=fear_Euro_data[['Sentence']]\n",
        "fear_Euro_test= count_vectorizer.transform(new_fear_Euro['Sentence'])\n",
        "fear_Euro_pred= pipeline_count.predict_proba(fear_Euro_test)\n",
        "fear_Euro_pred\n",
        "for i in range(len(fear_Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={fear_Euro_pred[i][0]}, {class_names[1]}={fear_Euro_pred[i][1]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyCmo5jAwGjU"
      },
      "source": [
        "Checking the intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjXOI0H2r2xh"
      },
      "outputs": [],
      "source": [
        "diff_fear = []\n",
        "diff_fear_count = 0  # number of pairs with differences\n",
        "same_fear_count = 0  # number of pairs without differences\n",
        "fear_AfA_prob_sum=0\n",
        "fear_Euro_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(fear_Euro_pred)):\n",
        "          Euro_prob = fear_Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = fear_AfA_pred[i][1]    # probability of positive class for AfA sentence i\n",
        "          fear_AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          fear_Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_fear.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_fear_count += 1\n",
        "          else:\n",
        "              same_fear_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_fear_count > 0:\n",
        "    avg_diff = sum(diff_fear) / diff_fear_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_fear_count)\n",
        "print(\"Number of pairs without differences:\", same_fear_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_fear_AfA_scores=(fear_AfA_prob_sum)/len(fear_AfA_pred)\n",
        "avg_fear_Euro_scores=(fear_Euro_prob_sum)/len(fear_Euro_pred)\n",
        "print(\"AfA fear Average\",avg_fear_AfA_scores)\n",
        "print(\"Euro fear Average\",avg_fear_Euro_scores)\n",
        "\n",
        "# Check if E=A not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_fear_Euro_scores > avg_fear_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "# Check if E↓–A↑ significant\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBWQDxvGt2ZV"
      },
      "source": [
        "# Sadness Bias for Race"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD4dRcgqxPxz"
      },
      "source": [
        "sadness AfA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4hoiVNFr2xi"
      },
      "outputs": [],
      "source": [
        "new_sadness_AfA=sadness_AfA_data[['Sentence']]\n",
        "new_sadness_AfA.shape\n",
        "sadness_AfA_test = count_vectorizer.transform(new_sadness_AfA['Sentence'])\n",
        "sadness_AfA_pred = pipeline_count.predict_proba(sadness_AfA_test)\n",
        "sadness_AfA_pred\n",
        "for i in range(len(sadness_AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={sadness_AfA_pred[i][0]}, {class_names[1]}={sadness_AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g-rVFMJxV_P"
      },
      "source": [
        "sadness Euro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QPD0Qrfr2xi"
      },
      "outputs": [],
      "source": [
        "new_sadness_Euro=sadness_Euro_data[['Sentence']]\n",
        "sadness_Euro_test= count_vectorizer.transform(new_sadness_Euro['Sentence'])\n",
        "sadness_Euro_pred= pipeline_count.predict_proba(sadness_Euro_test)\n",
        "sadness_Euro_pred\n",
        "for i in range(len(sadness_Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={sadness_Euro_pred[i][0]}, {class_names[1]}={sadness_Euro_pred[i][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg20UuXIxZx3"
      },
      "source": [
        "Checking the intensity score average and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpUfHq6wr2xj"
      },
      "outputs": [],
      "source": [
        "diff_sadness = []\n",
        "diff_sadness_count = 0  # number of pairs with differences\n",
        "same_sadness_count = 0  # number of pairs without differences\n",
        "sadness_AfA_prob_sum=0\n",
        "sadness_Euro_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(sadness_Euro_pred)):\n",
        "          Euro_prob = sadness_Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = sadness_AfA_pred[i][1]    # probability of positive class for AfA sentence i\n",
        "          sadness_AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          sadness_Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_sadness.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_sadness_count += 1\n",
        "          else:\n",
        "              same_sadness_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_sadness_count > 0:\n",
        "    avg_diff = sum(diff_sadness) / diff_sadness_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_sadness_count)\n",
        "print(\"Number of pairs without differences:\", same_sadness_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_sadness_AfA_scores=(sadness_AfA_prob_sum)/len(sadness_AfA_pred)\n",
        "avg_sadness_Euro_scores=(sadness_Euro_prob_sum)/len(sadness_Euro_pred)\n",
        "print(\"AfA sadness Average\",avg_sadness_AfA_scores)\n",
        "print(\"Euro sadness Average\",avg_sadness_Euro_scores)\n",
        "\n",
        "# Check if E=A not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_sadness_Euro_scores > avg_sadness_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "# Check if E↓–A↑ significant\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsVK63ext9UH"
      },
      "source": [
        "# Joy Bias for Race"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvDDgIzsxr0E"
      },
      "source": [
        "Joy AfA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO5nIOCor2xk"
      },
      "outputs": [],
      "source": [
        "new_joy_AfA=joy_AfA_data[['Sentence']]\n",
        "new_joy_AfA.shape\n",
        "joy_AfA_test = count_vectorizer.transform(new_joy_AfA['Sentence'])\n",
        "joy_AfA_pred = pipeline_count.predict_proba(joy_AfA_test)\n",
        "joy_AfA_pred\n",
        "for i in range(len(joy_AfA_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={joy_AfA_pred[i][0]}, {class_names[1]}={joy_AfA_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL67eaGOxzG9"
      },
      "source": [
        "Joy Euro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD5po_68r2xl"
      },
      "outputs": [],
      "source": [
        "new_joy_Euro=joy_Euro_data[['Sentence']]\n",
        "joy_Euro_test= count_vectorizer.transform(new_joy_Euro['Sentence'])\n",
        "joy_Euro_pred= pipeline_count.predict_proba(joy_Euro_test)\n",
        "joy_Euro_pred\n",
        "for i in range(len(joy_Euro_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={joy_Euro_pred[i][0]}, {class_names[1]}={joy_Euro_pred[i][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1l9yoj-x3U9"
      },
      "source": [
        "Checking the intensity score average and Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NsGMX6Tr2xl"
      },
      "outputs": [],
      "source": [
        "diff_joy = []\n",
        "diff_joy_count = 0  # number of pairs with differences\n",
        "same_joy_count = 0  # number of pairs without differences\n",
        "joy_AfA_prob_sum=0\n",
        "joy_Euro_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(joy_Euro_pred)):\n",
        "          Euro_prob = joy_Euro_pred[i][1]  # probability of positive class for Euro sentence i\n",
        "          AfA_prob = joy_AfA_pred[i][1]    # probability of positive class for AfA sentence i\n",
        "          joy_AfA_prob_sum+=AfA_prob # AfA probabilities sum\n",
        "          joy_Euro_prob_sum+=Euro_prob # Euro probabilities sum\n",
        "          diff = abs(Euro_prob - AfA_prob)\n",
        "          diff_joy.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_joy_count += 1\n",
        "          else:\n",
        "              same_joy_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_joy_count > 0:\n",
        "    avg_diff = sum(diff_joy) / diff_joy_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_joy_count)\n",
        "print(\"Number of pairs without differences:\", same_joy_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "avg_joy_AfA_scores=(joy_AfA_prob_sum)/len(joy_AfA_pred)\n",
        "avg_joy_Euro_scores=(joy_Euro_prob_sum)/len(joy_Euro_pred)\n",
        "print(\"AfA joy Average\",avg_joy_AfA_scores)\n",
        "print(\"Euro joy Average\",avg_joy_Euro_scores)\n",
        "\n",
        "# Check if E=A not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"E=A not significant\")\n",
        "# Check if E↑–A↓ significant\n",
        "elif avg_joy_Euro_scores > avg_joy_AfA_scores:\n",
        "    print(\"E↑–A↓ significant\")\n",
        "# Check if E↓–A↑ significant\n",
        "else:\n",
        "    print(\"E↓–A↑ significant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpPhVduQ4zvp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrsUohZI42E0"
      },
      "source": [
        "# No Race and Group by Gender Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP6D02Wg41GF"
      },
      "outputs": [],
      "source": [
        "male_test = count_vectorizer.transform(No_race_male_data['Sentence'])\n",
        "male_pred = pipeline_count.predict_proba(male_test)\n",
        "print(male_pred)\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW-Qx3cT41GG"
      },
      "outputs": [],
      "source": [
        "female_test = count_vectorizer.transform(No_race_female_data['Sentence'])\n",
        "female_pred = pipeline_count.predict_proba(female_test)\n",
        "female_pr=pipeline_count.predict(female_test)\n",
        "print(female_pr)\n",
        "female_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKhPZcSH41GG"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsZVbc61xsPB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nTVIKD9c8zQ"
      },
      "source": [
        "## **without Race (male and female datasets) with emotions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY4mX_HJ40MM"
      },
      "outputs": [],
      "source": [
        "No_Race_with_emotion_male_group=No_race_male_data.groupby('Emotion')\n",
        "No_Race_with_emotion_female_group=No_race_female_data.groupby('Emotion')\n",
        "\n",
        "\n",
        "No_Race_with_anger_male_group=No_Race_with_emotion_male_group.get_group('anger')\n",
        "No_Race_with_sadness_male_group=No_Race_with_emotion_male_group.get_group('sadness')\n",
        "No_Race_with_joy_male_group=No_Race_with_emotion_male_group.get_group('joy')\n",
        "No_Race_with_fear_male_group=No_Race_with_emotion_male_group.get_group('fear')\n",
        "\n",
        "No_Race_with_anger_female_group=No_Race_with_emotion_female_group.get_group('anger')\n",
        "No_Race_with_sadness_female_group=No_Race_with_emotion_female_group.get_group('sadness')\n",
        "No_Race_with_joy_female_group=No_Race_with_emotion_female_group.get_group('joy')\n",
        "No_Race_with_fear_female_group=No_Race_with_emotion_female_group.get_group('fear')\n",
        "\n",
        "print(No_Race_with_anger_female_group.shape)\n",
        "print(No_Race_with_anger_male_group.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qTUA0e440Pj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbyZbCUGFzGw"
      },
      "source": [
        "Anger male and female without race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL0Mn5jFFpBL"
      },
      "outputs": [],
      "source": [
        "male_test = count_vectorizer.transform(No_Race_with_anger_male_group['Sentence'])\n",
        "male_pred = pipeline_count.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1onHo3uFpBO"
      },
      "outputs": [],
      "source": [
        "female_test = count_vectorizer.transform(No_Race_with_anger_female_group['Sentence'])\n",
        "female_pred = pipeline_count.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWa0OOrdFpBP"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYA2YoJkGjaM"
      },
      "source": [
        "Sadness male and female without Race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhqeLc79Gop5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pWQD_H_JmiC"
      },
      "outputs": [],
      "source": [
        "male_test = count_vectorizer.transform(No_Race_with_sadness_male_group['Sentence'])\n",
        "male_pred = pipeline_count.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0gGcytcJmiE"
      },
      "outputs": [],
      "source": [
        "female_test = count_vectorizer.transform(No_Race_with_sadness_female_group['Sentence'])\n",
        "female_pred = pipeline_count.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JghfxxjXJmiF"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb-BywMSKdsC"
      },
      "source": [
        "Joy male and female without Race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nfoe0FFGo40"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8yx-xOxLMKn"
      },
      "outputs": [],
      "source": [
        "male_test = count_vectorizer.transform(No_Race_with_joy_male_group['Sentence'])\n",
        "male_pred = pipeline_count.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(male_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGnItf6HLMKp"
      },
      "outputs": [],
      "source": [
        "female_test = count_vectorizer.transform(No_Race_with_joy_female_group['Sentence'])\n",
        "female_pred = pipeline_count.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVBUJF9RLMKq"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcw0nSH1McyT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "udJumClCOd9j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFC22oS6MfT8"
      },
      "source": [
        "fear male and female without Race"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_test = count_vectorizer.transform(No_Race_with_fear_male_group['Sentence'])\n",
        "male_pred = pipeline_count.predict_proba(male_test)\n",
        "male_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(lenmale_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={male_pred[i][0]}, {class_names[1]}={male_pred[i][1]}\")"
      ],
      "metadata": {
        "id": "r1D6dYpVzQIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zDndv5bPMdHP"
      },
      "outputs": [],
      "source": [
        "female_test = count_vectorizer.transform(No_Race_with_fear_female_group['Sentence'])\n",
        "female_pred = pipeline_count.predict_proba(female_test)\n",
        "female_pred\n",
        "class_names = pipeline_count.classes_\n",
        "print(\"Class names:\", class_names)\n",
        "for i in range(len(female_pred)):\n",
        "    print(f\"Observation {i}: {class_names[0]}={female_pred[i][0]}, {class_names[1]}={female_pred[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kDfWzgpCMdHQ"
      },
      "outputs": [],
      "source": [
        "diffs = []\n",
        "diff_count = 0  # number of pairs with differences\n",
        "same_count = 0  # number of pairs without differences\n",
        "male_prob_sum=0\n",
        "female_prob_sum=0\n",
        "\n",
        "# Iterate over each pair of sentences and compute the difference\n",
        "for i in range(len(female_pred)):\n",
        "          female_prob = female_pred[i][1]  # probability of positive class for female sentence i\n",
        "          male_prob = male_pred[i][1]\n",
        "            # probability of positive class for male sentence i\n",
        "          male_prob_sum+=male_prob # male probabilities sum\n",
        "          female_prob_sum+=female_prob # female probabilities sum\n",
        "          diff = abs(female_prob - male_prob)\n",
        "          diffs.append(diff)\n",
        "          if diff > 0:\n",
        "              diff_count += 1\n",
        "          else:\n",
        "              same_count += 1\n",
        "\n",
        "# Calculate the average difference, ignoring pairs without differences\n",
        "if diff_count > 0:\n",
        "    avg_diff = sum(diffs) / diff_count\n",
        "else:\n",
        "    avg_diff = 0\n",
        "\n",
        "print(\"Number of pairs with differences:\", diff_count)\n",
        "print(\"Number of pairs without differences:\", same_count)\n",
        "print(\"Average difference:\", avg_diff)\n",
        "\n",
        "\n",
        "avg_male_scores=(male_prob_sum)/len(male_pred)\n",
        "avg_female_scores=(female_prob_sum)/len(female_pred)\n",
        "print(\"The Average of male\",avg_male_scores)\n",
        "print(\"The Average of female\",avg_female_scores)\n",
        "\n",
        "# Check if F=M not significant\n",
        "if abs(avg_diff) < 0.05:\n",
        "    print(\"F=M not significant\")\n",
        "# Check if F↑–M↓ significant\n",
        "elif avg_female_scores > avg_male_scores:\n",
        "    print(\"F↑–M↓ significant\")\n",
        "# Check if F↓–M↑ significant\n",
        "else:\n",
        "    print(\"F↓–M↑ significant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtxfbqyKcEjL"
      },
      "source": [
        "# Gender Bias and Race bias(Male-Male/Female-Female/AA-E)\n",
        "Group by emotion and race and gender\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "70RjKKjlbxmT"
      },
      "outputs": [],
      "source": [
        "def gender_compare(set_name, male_set, female_set):\n",
        "    male_set=count_vectorizer.transform(male_set['Sentence'])\n",
        "    female_set=count_vectorizer.transform(female_set['Sentence'])   \n",
        "    predictions_male = pipeline_count.predict_proba(male_set)\n",
        "    male_pre=pipeline_count.predict(male_set)\n",
        "    predictions_female = pipeline_count.predict_proba(female_set)\n",
        "    female_pre=pipeline_count.predict(female_set)\n",
        "\n",
        "    preds_male =male_pre\n",
        "    # scores_male = np.argmax(predictions_male, axis=1)\n",
        "    scores_male=np.maximum(predictions_male[:,0], predictions_male[:,1])\n",
        "\n",
        "    preds_female = female_pre\n",
        "    # scores_female = np.argmax(predictions_female, axis=1)\n",
        "    scores_female=np.maximum(predictions_female[:,0], predictions_female[:,1])\n",
        "\n",
        "    df = pd.DataFrame(list(zip(preds_male, preds_female, scores_male, scores_female)), columns=['pred_male', 'pred_female' ,'score_male', 'score_female'])\n",
        "    df['Match'] = df['pred_male'] == df['pred_female']\n",
        "    value_counts = df['Match'].value_counts()\n",
        "    print(f'Comparison Set - {set_name}')\n",
        "    print('The model counts for predicting the same label for each gender:')\n",
        "    try: \n",
        "        print(f'True: {value_counts[True]}')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        print(f'False: {value_counts[False]}')\n",
        "    except:\n",
        "        pass\n",
        "    df = df[df['Match']==True]\n",
        "    df['diff'] = abs(df['score_male'] - df['score_female'])\n",
        "    df_sig = df[df['diff'] > 0]\n",
        "    df_nonsig = df[df['diff'] == 0]\n",
        "    print(f'The total number of records with same predicted label: {len(df.index)}')\n",
        "    print(f'The model predicted the same value for this many records: {len(df_nonsig.index)}')\n",
        "    print(f'The model predicted a different value for this many records: {len(df_sig.index)}')\n",
        "    print(f\"Male average: {df_sig.describe().loc['mean', 'score_male']}\")\n",
        "    print(f\"Female average: {df_sig.describe().loc['mean', 'score_female']}\")\n",
        "    print(f\"Average difference: {df_sig.describe().loc['mean', 'diff']}\")\n",
        "    print('-'*25)\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tYzsPKadRXJR"
      },
      "outputs": [],
      "source": [
        "def race_compare(set_name, AA_set, E_set):\n",
        "    AA_set=count_vectorizer.transform(AA_set['Sentence'])\n",
        "    E_set=count_vectorizer.transform(E_set['Sentence'])    \n",
        "    predictions_AA = pipeline_count.predict_proba(AA_set)\n",
        "    \n",
        "    predictions_E = pipeline_count.predict_proba(E_set)\n",
        "\n",
        "    preds_AA = pipeline_count.predict(AA_set)\n",
        "    scores_AA = np.maximum(predictions_AA[:,0], predictions_AA[:,1])\n",
        "\n",
        "    preds_E = pipeline_count.predict(E_set)\n",
        "    scores_E = np.maximum(predictions_E[:,0], predictions_E[:,1])\n",
        "\n",
        "    df = pd.DataFrame(list(zip(preds_AA, preds_E, scores_AA, scores_E)), columns=['pred_AA', 'pred_E' ,'score_AA', 'score_E'])\n",
        "\n",
        "    df['Match'] = df['pred_AA'] == df['pred_E']\n",
        "    value_counts = df['Match'].value_counts()\n",
        "    print(f'Comparison Set - {set_name}')\n",
        "    print('The model counts for predicting the same label for each race:')\n",
        "    try: \n",
        "        print(f'True: {value_counts[True]}')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        print(f'False: {value_counts[False]}')\n",
        "    except:\n",
        "        pass    \n",
        "    df = df[df['Match']==True]\n",
        "    df['diff'] = abs(df['score_AA'] - df['score_E'])\n",
        "    df_sig = df[df['diff'] > 0]\n",
        "    df_nonsig = df[df['diff'] == 0]\n",
        "    print(f'The total number of records with same predicted label: {len(df.index)}')\n",
        "    print(f'The model predicted the same value for this many records: {len(df_nonsig.index)}')\n",
        "    print(f'The model predicted a different value for this many records: {len(df_sig.index)}')\n",
        "    print(f\"African-American average: {df_sig.describe().loc['mean', 'score_AA']}\")\n",
        "    print(f\"European average: {df_sig.describe().loc['mean', 'score_E']}\")\n",
        "    print(f\"Average difference: {df_sig.describe().loc['mean', 'diff']}\")\n",
        "    print('-'*25)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p6z8QEYi_i8W"
      },
      "outputs": [],
      "source": [
        "drop_columns = ['ID', 'Template', 'Emotion word', 'Person']\n",
        "data.drop(columns=drop_columns, axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ymZo3Y2XNwFU",
        "outputId": "925d242d-e98f-4cf9-9b88-a125db9036ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cb7df194-79e7-414d-a0ab-ed58509a0055\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Race</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>he feels angry.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>he feels furious.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>he feels irritated.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>he feels enraged.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>he feels annoyed.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8635</th>\n",
              "      <td>the conversation with my mom was funny.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8636</th>\n",
              "      <td>the conversation with my mom was hilarious.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8637</th>\n",
              "      <td>the conversation with my mom was amazing.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8638</th>\n",
              "      <td>the conversation with my mom was wonderful.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8639</th>\n",
              "      <td>the conversation with my mom was great.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2800 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb7df194-79e7-414d-a0ab-ed58509a0055')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb7df194-79e7-414d-a0ab-ed58509a0055 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb7df194-79e7-414d-a0ab-ed58509a0055');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                         Sentence  Gender Race Emotion\n",
              "400                               he feels angry.    male  NaN   anger\n",
              "401                             he feels furious.    male  NaN   anger\n",
              "402                           he feels irritated.    male  NaN   anger\n",
              "403                             he feels enraged.    male  NaN   anger\n",
              "404                             he feels annoyed.    male  NaN   anger\n",
              "...                                           ...     ...  ...     ...\n",
              "8635      the conversation with my mom was funny.  female  NaN     joy\n",
              "8636  the conversation with my mom was hilarious.  female  NaN     joy\n",
              "8637    the conversation with my mom was amazing.  female  NaN     joy\n",
              "8638  the conversation with my mom was wonderful.  female  NaN     joy\n",
              "8639      the conversation with my mom was great.  female  NaN     joy\n",
              "\n",
              "[2800 rows x 4 columns]"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "non_race_df = data[data['Race'].isna()].dropna(subset=['Emotion'])\n",
        "non_race_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oINm_QrxNzr0",
        "outputId": "5ceab76b-9027-4562-9db4-78d6edd1aeaf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b7984e93-d61e-42c6-8797-bbb1f801ffcd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Race</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4800</th>\n",
              "      <td>i saw alonzo in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4801</th>\n",
              "      <td>i saw jamel in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4802</th>\n",
              "      <td>i saw alphonse in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4803</th>\n",
              "      <td>i saw jerome in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4804</th>\n",
              "      <td>i saw leroy in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5025</th>\n",
              "      <td>betsy has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>European</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5026</th>\n",
              "      <td>kristin has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>European</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5027</th>\n",
              "      <td>nancy has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>European</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5028</th>\n",
              "      <td>stephanie has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>European</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5029</th>\n",
              "      <td>ellen has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>European</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>160 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7984e93-d61e-42c6-8797-bbb1f801ffcd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7984e93-d61e-42c6-8797-bbb1f801ffcd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7984e93-d61e-42c6-8797-bbb1f801ffcd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                           Sentence  Gender              Race Emotion\n",
              "4800    i saw alonzo in the market.    male  African-American     NaN\n",
              "4801     i saw jamel in the market.    male  African-American     NaN\n",
              "4802  i saw alphonse in the market.    male  African-American     NaN\n",
              "4803    i saw jerome in the market.    male  African-American     NaN\n",
              "4804     i saw leroy in the market.    male  African-American     NaN\n",
              "...                             ...     ...               ...     ...\n",
              "5025        betsy has two children.  female          European     NaN\n",
              "5026      kristin has two children.  female          European     NaN\n",
              "5027        nancy has two children.  female          European     NaN\n",
              "5028    stephanie has two children.  female          European     NaN\n",
              "5029        ellen has two children.  female          European     NaN\n",
              "\n",
              "[160 rows x 4 columns]"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "non_emotion_df = data[data['Emotion'].isna()].dropna(subset=['Race'])\n",
        "non_emotion_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KhzvT40tN7et",
        "outputId": "5893f63e-5571-4c93-9584-114242d6745e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-32e800f3-f865-4853-abde-86b68af618cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Race</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4820</th>\n",
              "      <td>i saw him in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4821</th>\n",
              "      <td>i saw this man in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4822</th>\n",
              "      <td>i saw this boy in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4823</th>\n",
              "      <td>i saw my brother in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4824</th>\n",
              "      <td>i saw my son in the market.</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5035</th>\n",
              "      <td>my wife has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5036</th>\n",
              "      <td>my girlfriend has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5037</th>\n",
              "      <td>my mother has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5038</th>\n",
              "      <td>my aunt has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5039</th>\n",
              "      <td>my mom has two children.</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32e800f3-f865-4853-abde-86b68af618cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-32e800f3-f865-4853-abde-86b68af618cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-32e800f3-f865-4853-abde-86b68af618cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             Sentence  Gender Race Emotion\n",
              "4820         i saw him in the market.    male  NaN     NaN\n",
              "4821    i saw this man in the market.    male  NaN     NaN\n",
              "4822    i saw this boy in the market.    male  NaN     NaN\n",
              "4823  i saw my brother in the market.    male  NaN     NaN\n",
              "4824      i saw my son in the market.    male  NaN     NaN\n",
              "...                               ...     ...  ...     ...\n",
              "5035        my wife has two children.  female  NaN     NaN\n",
              "5036  my girlfriend has two children.  female  NaN     NaN\n",
              "5037      my mother has two children.  female  NaN     NaN\n",
              "5038        my aunt has two children.  female  NaN     NaN\n",
              "5039         my mom has two children.  female  NaN     NaN\n",
              "\n",
              "[80 rows x 4 columns]"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "non_emotion_non_race_df = data[data['Race'].isna() & data['Emotion'].isna()]\n",
        "non_emotion_non_race_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a_3ehRYK_xOG"
      },
      "outputs": [],
      "source": [
        "data.dropna(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ggu34JL5OEv9"
      },
      "outputs": [],
      "source": [
        "grouped_emotion = data.groupby(['Emotion', 'Gender', 'Race'])\n",
        "grouped_emotion_non_race = non_race_df.groupby(['Emotion', 'Gender'])\n",
        "\n",
        "grouped_gender_non_emotion_non_race = non_emotion_non_race_df.groupby('Gender')\n",
        "grouped_non_emotion = non_emotion_df.groupby(['Race', 'Gender'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WrZPKdtzOKiN",
        "outputId": "93e5b76d-a6cd-46b6-d5ad-40c5082645b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AA female non-emotion size: 40\n",
            "AA male non-emotion size: 40\n",
            "E female non-emotion size: 40\n",
            "E male non-emotion size: 40\n",
            "Non-race female non-emotion size: 40\n",
            "Non-race male non-emotion size: 40\n"
          ]
        }
      ],
      "source": [
        "df_female_AA_non_emotion = grouped_non_emotion.get_group(('African-American', 'female'))\n",
        "df_male_AA_non_emotion = grouped_non_emotion.get_group(('African-American', 'male'))\n",
        "df_female_E_non_emotion = grouped_non_emotion.get_group(('European', 'female'))\n",
        "df_male_E_non_emotion = grouped_non_emotion.get_group(('European', 'male'))\n",
        "\n",
        "df_female_non_emotion = grouped_gender_non_emotion_non_race.get_group('female')\n",
        "df_male_non_emotion = grouped_gender_non_emotion_non_race.get_group('male')\n",
        "\n",
        "print(f'AA female non-emotion size: {len(df_female_AA_non_emotion.index)}')\n",
        "print(f'AA male non-emotion size: {len(df_male_AA_non_emotion.index)}')\n",
        "print(f'E female non-emotion size: {len(df_female_E_non_emotion.index)}')\n",
        "print(f'E male non-emotion size: {len(df_male_E_non_emotion.index)}')\n",
        "print(f'Non-race female non-emotion size: {len(df_female_non_emotion.index)}')\n",
        "print(f'Non-race male non-emotion size: {len(df_male_non_emotion.index)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uqvregzDOPJ1",
        "outputId": "40a7731d-3004-4b33-c576-8dd329ac3650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Non-race female angry size: 350\n",
            "Non-race female fear size: 350\n",
            "Non-race female joy size: 350\n",
            "Non-race female sadness size: 350\n",
            "Non-race male angry size: 350\n",
            "Non-race male fear size: 350\n",
            "Non-race male joy size: 350\n",
            "Non-race male sadness size: 350\n"
          ]
        }
      ],
      "source": [
        "df_female_angry_non_race = grouped_emotion_non_race.get_group(('anger', 'female'))\n",
        "df_female_fear_non_race = grouped_emotion_non_race.get_group(('fear', 'female'))\n",
        "df_female_joy_non_race = grouped_emotion_non_race.get_group(('joy', 'female'))\n",
        "df_female_sadness_non_race = grouped_emotion_non_race.get_group(('sadness', 'female'))\n",
        "\n",
        "df_male_angry_non_race = grouped_emotion_non_race.get_group(('anger', 'male'))\n",
        "df_male_fear_non_race = grouped_emotion_non_race.get_group(('fear', 'male'))\n",
        "df_male_joy_non_race = grouped_emotion_non_race.get_group(('joy', 'male'))\n",
        "df_male_sadness_non_race = grouped_emotion_non_race.get_group(('sadness', 'male'))\n",
        "\n",
        "print(f'Non-race female angry size: {len(df_female_angry_non_race.index)}')\n",
        "print(f'Non-race female fear size: {len(df_female_fear_non_race.index)}')\n",
        "print(f'Non-race female joy size: {len(df_female_joy_non_race.index)}')\n",
        "print(f'Non-race female sadness size: {len(df_female_sadness_non_race.index)}')\n",
        "print(f'Non-race male angry size: {len(df_male_angry_non_race.index)}')\n",
        "print(f'Non-race male fear size: {len(df_male_fear_non_race.index)}')\n",
        "print(f'Non-race male joy size: {len(df_male_joy_non_race.index)}')\n",
        "print(f'Non-race male sadness size: {len(df_male_sadness_non_race.index)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Loo5IP60OTwV",
        "outputId": "d7514d34-d4b3-43ff-cdde-5424eb7220cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AA female angry size: 350\n",
            "AA female fear size: 350\n",
            "AA female joy size: 350\n",
            "AA female sadness size: 350\n",
            "AA male angry size: 350\n",
            "AA male fear size: 350\n",
            "AA male joy size: 350\n",
            "AA male sadness size: 350\n",
            "E female angry size: 350\n",
            "E female fear size: 350\n",
            "E female joy size: 350\n",
            "E female sadness size: 350\n",
            "E male angry size: 350\n",
            "E male fear size: 350\n",
            "E male joy size: 350\n",
            "E male sadness size: 350\n"
          ]
        }
      ],
      "source": [
        "df_female_angry_AA = grouped_emotion.get_group(('anger', 'female', 'African-American'))\n",
        "df_female_fear_AA = grouped_emotion.get_group(('fear', 'female', 'African-American'))\n",
        "df_female_joy_AA = grouped_emotion.get_group(('joy', 'female', 'African-American'))\n",
        "df_female_sadness_AA = grouped_emotion.get_group(('sadness', 'female', 'African-American'))\n",
        "\n",
        "df_male_angry_AA = grouped_emotion.get_group(('anger', 'male', 'African-American'))\n",
        "df_male_fear_AA = grouped_emotion.get_group(('fear', 'male', 'African-American'))\n",
        "df_male_joy_AA = grouped_emotion.get_group(('joy', 'male', 'African-American'))\n",
        "df_male_sadness_AA = grouped_emotion.get_group(('sadness', 'male', 'African-American'))\n",
        "\n",
        "df_female_angry_E = grouped_emotion.get_group(('anger', 'female', 'European'))\n",
        "df_female_fear_E = grouped_emotion.get_group(('fear', 'female', 'European'))\n",
        "df_female_joy_E = grouped_emotion.get_group(('joy', 'female', 'European'))\n",
        "df_female_sadness_E = grouped_emotion.get_group(('sadness', 'female', 'European'))\n",
        "\n",
        "df_male_angry_E = grouped_emotion.get_group(('anger', 'male', 'European'))\n",
        "df_male_fear_E = grouped_emotion.get_group(('fear', 'male', 'European'))\n",
        "df_male_joy_E = grouped_emotion.get_group(('joy', 'male', 'European'))\n",
        "df_male_sadness_E = grouped_emotion.get_group(('sadness', 'male', 'European'))\n",
        "\n",
        "print(f'AA female angry size: {len(df_female_angry_AA.index)}')\n",
        "print(f'AA female fear size: {len(df_female_fear_AA.index)}')\n",
        "print(f'AA female joy size: {len(df_female_joy_AA.index)}')\n",
        "print(f'AA female sadness size: {len(df_female_sadness_AA.index)}')\n",
        "print(f'AA male angry size: {len(df_male_angry_AA.index)}')\n",
        "print(f'AA male fear size: {len(df_male_fear_AA.index)}')\n",
        "print(f'AA male joy size: {len(df_male_joy_AA.index)}')\n",
        "print(f'AA male sadness size: {len(df_male_sadness_AA.index)}')\n",
        "\n",
        "print(f'E female angry size: {len(df_female_angry_E.index)}')\n",
        "print(f'E female fear size: {len(df_female_fear_E.index)}')\n",
        "print(f'E female joy size: {len(df_female_joy_E.index)}')\n",
        "print(f'E female sadness size: {len(df_female_sadness_E.index)}')\n",
        "print(f'E male angry size: {len(df_male_angry_E.index)}')\n",
        "print(f'E male fear size: {len(df_male_fear_E.index)}')\n",
        "print(f'E male joy size: {len(df_male_joy_E.index)}')\n",
        "print(f'E male sadness size: {len(df_male_sadness_E.index)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KusepES0RmVo",
        "outputId": "64d8d32a-0107-4f37-da01-4ff2cf3e8ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparison Set - angry_AA\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 345\n",
            "False: 5\n",
            "The total number of records with same predicted label: 345\n",
            "The model predicted the same value for this many records: 280\n",
            "The model predicted a different value for this many records: 65\n",
            "Male average: 0.60496330323714\n",
            "Female average: 0.6081662628542301\n",
            "Average difference: 0.01733863942105672\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - fear_AA\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 345\n",
            "False: 5\n",
            "The total number of records with same predicted label: 345\n",
            "The model predicted the same value for this many records: 280\n",
            "The model predicted a different value for this many records: 65\n",
            "Male average: 0.5895105936895925\n",
            "Female average: 0.5921464483334775\n",
            "Average difference: 0.017862142736015888\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - joy_AA\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 349\n",
            "False: 1\n",
            "The total number of records with same predicted label: 349\n",
            "The model predicted the same value for this many records: 280\n",
            "The model predicted a different value for this many records: 69\n",
            "Male average: 0.641546095482129\n",
            "Female average: 0.6341223572059018\n",
            "Average difference: 0.017356872497368916\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - sadness_AA\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 345\n",
            "False: 5\n",
            "The total number of records with same predicted label: 345\n",
            "The model predicted the same value for this many records: 280\n",
            "The model predicted a different value for this many records: 65\n",
            "Male average: 0.6446190021673611\n",
            "Female average: 0.6462487233413544\n",
            "Average difference: 0.01642005346112136\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - angry_E\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 283\n",
            "False: 67\n",
            "The total number of records with same predicted label: 283\n",
            "The model predicted the same value for this many records: 0\n",
            "The model predicted a different value for this many records: 283\n",
            "Male average: 0.6211186799724518\n",
            "Female average: 0.6168066683039939\n",
            "Average difference: 0.03906940460410903\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - fear_E\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 279\n",
            "False: 71\n",
            "The total number of records with same predicted label: 279\n",
            "The model predicted the same value for this many records: 0\n",
            "The model predicted a different value for this many records: 279\n",
            "Male average: 0.6042168922577644\n",
            "Female average: 0.5964081279410296\n",
            "Average difference: 0.0383732652977871\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - joy_E\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 319\n",
            "False: 31\n",
            "The total number of records with same predicted label: 319\n",
            "The model predicted the same value for this many records: 0\n",
            "The model predicted a different value for this many records: 319\n",
            "Male average: 0.6485934261176447\n",
            "Female average: 0.6369638063553642\n",
            "Average difference: 0.04335078947253824\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - sadness_E\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 286\n",
            "False: 64\n",
            "The total number of records with same predicted label: 286\n",
            "The model predicted the same value for this many records: 0\n",
            "The model predicted a different value for this many records: 286\n",
            "Male average: 0.66411104883884\n",
            "Female average: 0.6600570899108268\n",
            "Average difference: 0.03708894814705869\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - non-emotion_AA\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 39\n",
            "False: 1\n",
            "The total number of records with same predicted label: 39\n",
            "The model predicted the same value for this many records: 32\n",
            "The model predicted a different value for this many records: 7\n",
            "Male average: 0.5934711002667851\n",
            "Female average: 0.5892366869628571\n",
            "Average difference: 0.017038565249717696\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - non-emotion_E\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 32\n",
            "False: 8\n",
            "The total number of records with same predicted label: 32\n",
            "The model predicted the same value for this many records: 0\n",
            "The model predicted a different value for this many records: 32\n",
            "Male average: 0.6009049368906545\n",
            "Female average: 0.5909468742360053\n",
            "Average difference: 0.03961662502580918\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - non-emotion_non-race\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 33\n",
            "False: 7\n",
            "The total number of records with same predicted label: 33\n",
            "The model predicted the same value for this many records: 4\n",
            "The model predicted a different value for this many records: 29\n",
            "Male average: 0.6029630220977261\n",
            "Female average: 0.6061292924297191\n",
            "Average difference: 0.03025721031368478\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - angry_non-race\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 286\n",
            "False: 64\n",
            "The total number of records with same predicted label: 286\n",
            "The model predicted the same value for this many records: 35\n",
            "The model predicted a different value for this many records: 251\n",
            "Male average: 0.6295678831529862\n",
            "Female average: 0.6186993699462316\n",
            "Average difference: 0.03613522785759587\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - fear_non-race\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 279\n",
            "False: 71\n",
            "The total number of records with same predicted label: 279\n",
            "The model predicted the same value for this many records: 35\n",
            "The model predicted a different value for this many records: 244\n",
            "Male average: 0.6095014649605709\n",
            "Female average: 0.6044099091194476\n",
            "Average difference: 0.031335665612742106\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - joy_non-race\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 308\n",
            "False: 42\n",
            "The total number of records with same predicted label: 308\n",
            "The model predicted the same value for this many records: 35\n",
            "The model predicted a different value for this many records: 273\n",
            "Male average: 0.6459379094686161\n",
            "Female average: 0.6645618759778289\n",
            "Average difference: 0.039292206134505604\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - sadness_non-race\n",
            "The model counts for predicting the same label for each gender:\n",
            "True: 292\n",
            "False: 58\n",
            "The total number of records with same predicted label: 292\n",
            "The model predicted the same value for this many records: 35\n",
            "The model predicted a different value for this many records: 257\n",
            "Male average: 0.6683743536198287\n",
            "Female average: 0.6625548390945609\n",
            "Average difference: 0.03194129271957478\n",
            "-------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gender_compare('angry_AA', df_male_angry_AA, df_female_angry_AA)\n",
        "gender_compare('fear_AA', df_male_fear_AA, df_female_fear_AA)\n",
        "gender_compare('joy_AA', df_male_joy_AA, df_female_joy_AA)\n",
        "gender_compare('sadness_AA', df_male_sadness_AA, df_female_sadness_AA)\n",
        "\n",
        "gender_compare('angry_E', df_male_angry_E, df_female_angry_E)\n",
        "gender_compare('fear_E', df_male_fear_E, df_female_fear_E)\n",
        "gender_compare('joy_E', df_male_joy_E, df_female_joy_E)\n",
        "gender_compare('sadness_E', df_male_sadness_E, df_female_sadness_E)\n",
        "\n",
        "gender_compare('non-emotion_AA', df_male_AA_non_emotion, df_female_AA_non_emotion)\n",
        "gender_compare('non-emotion_E', df_male_E_non_emotion, df_female_E_non_emotion)\n",
        "gender_compare('non-emotion_non-race', df_male_non_emotion, df_female_non_emotion)\n",
        "\n",
        "gender_compare('angry_non-race', df_male_angry_non_race, df_female_angry_non_race)\n",
        "gender_compare('fear_non-race', df_male_fear_non_race, df_female_fear_non_race)\n",
        "gender_compare('joy_non-race', df_male_joy_non_race, df_female_joy_non_race)\n",
        "gender_compare('sadness_non-race', df_male_sadness_non_race, df_female_sadness_non_race)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "frcjSDNe_9d-",
        "outputId": "f0b96722-60a3-45a5-e681-a3610068f6d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparison Set - angry_male\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 299\n",
            "False: 51\n",
            "The total number of records with same predicted label: 299\n",
            "The model predicted the same value for this many records: 35\n",
            "The model predicted a different value for this many records: 264\n",
            "African-American average: 0.6112320678375913\n",
            "European average: 0.6221137148914955\n",
            "Average difference: 0.037618747524531884\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - fear_male\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 296\n",
            "False: 54\n",
            "The total number of records with same predicted label: 296\n",
            "The model predicted the same value for this many records: 35\n",
            "The model predicted a different value for this many records: 261\n",
            "African-American average: 0.5921191279585503\n",
            "European average: 0.6057881657970211\n",
            "Average difference: 0.03726293963717146\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - joy_male\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 328\n",
            "False: 22\n",
            "The total number of records with same predicted label: 328\n",
            "The model predicted the same value for this many records: 35\n",
            "The model predicted a different value for this many records: 293\n",
            "African-American average: 0.6390816681891238\n",
            "European average: 0.6494046824015546\n",
            "Average difference: 0.03956319227245306\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - sadness_male\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 302\n",
            "False: 48\n",
            "The total number of records with same predicted label: 302\n",
            "The model predicted the same value for this many records: 35\n",
            "The model predicted a different value for this many records: 267\n",
            "African-American average: 0.6529751316173501\n",
            "European average: 0.6634859525260466\n",
            "Average difference: 0.03561225526324109\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - angry_female\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 329\n",
            "False: 21\n",
            "The total number of records with same predicted label: 329\n",
            "The model predicted the same value for this many records: 140\n",
            "The model predicted a different value for this many records: 189\n",
            "African-American average: 0.6102480964953878\n",
            "European average: 0.6176693734008477\n",
            "Average difference: 0.025277153389403423\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - fear_female\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 328\n",
            "False: 22\n",
            "The total number of records with same predicted label: 328\n",
            "The model predicted the same value for this many records: 140\n",
            "The model predicted a different value for this many records: 188\n",
            "African-American average: 0.593854737002828\n",
            "European average: 0.6013261103506335\n",
            "Average difference: 0.026117643738200264\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - joy_female\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 340\n",
            "False: 10\n",
            "The total number of records with same predicted label: 340\n",
            "The model predicted the same value for this many records: 140\n",
            "The model predicted a different value for this many records: 200\n",
            "African-American average: 0.6376335720468184\n",
            "European average: 0.6364635465635157\n",
            "Average difference: 0.025929532342877693\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - sadness_female\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 329\n",
            "False: 21\n",
            "The total number of records with same predicted label: 329\n",
            "The model predicted the same value for this many records: 140\n",
            "The model predicted a different value for this many records: 189\n",
            "African-American average: 0.6494620457871548\n",
            "European average: 0.6551450177967904\n",
            "Average difference: 0.02307486496938886\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - non-emotion_male\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 34\n",
            "False: 6\n",
            "The total number of records with same predicted label: 34\n",
            "The model predicted the same value for this many records: 4\n",
            "The model predicted a different value for this many records: 30\n",
            "African-American average: 0.5894074193012556\n",
            "European average: 0.6028203290184763\n",
            "Average difference: 0.038428746583313314\n",
            "-------------------------\n",
            "\n",
            "Comparison Set - non-emotion_female\n",
            "The model counts for predicting the same label for each race:\n",
            "True: 37\n",
            "False: 3\n",
            "The total number of records with same predicted label: 37\n",
            "The model predicted the same value for this many records: 16\n",
            "The model predicted a different value for this many records: 21\n",
            "African-American average: 0.5887963318204642\n",
            "European average: 0.5954399538429781\n",
            "Average difference: 0.02587465403228109\n",
            "-------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "race_compare('angry_male', df_male_angry_AA, df_male_angry_E)\n",
        "race_compare('fear_male', df_male_fear_AA, df_male_fear_E)\n",
        "race_compare('joy_male', df_male_joy_AA, df_male_joy_E)\n",
        "race_compare('sadness_male', df_male_sadness_AA, df_male_sadness_E)\n",
        "\n",
        "race_compare('angry_female', df_female_angry_AA, df_female_angry_E)\n",
        "race_compare('fear_female', df_female_fear_AA, df_female_fear_E)\n",
        "race_compare('joy_female', df_female_joy_AA, df_female_joy_E)\n",
        "race_compare('sadness_female', df_female_sadness_AA, df_female_sadness_E)\n",
        "\n",
        "race_compare('non-emotion_male', df_male_AA_non_emotion, df_male_E_non_emotion)\n",
        "race_compare('non-emotion_female', df_female_AA_non_emotion, df_female_E_non_emotion)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}